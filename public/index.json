[{"authors":null,"categories":null,"content":"\u0026hellip;\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"/privacy/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/privacy/","section":"","summary":"\u0026hellip;","tags":null,"title":"Privacy Policy","type":"page"},{"authors":["Damien Dupré","Nicole Andelic","Anna Zajac","Gawain Morrison","Gary McKeown"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"b310d181aea2f2e2023bacd44f0cda8a","permalink":"/publication/2018-socialmedia-psyarxiv/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/2018-socialmedia-psyarxiv/","section":"publication","summary":"Sharing personal information is an important way of communicating on social media. Among the information possibly shared, new sensors and tools allow people to share emotion information via facial emotion recognition. This paper questions whether people are prepared to share personal information such as their own emotion on social media. In the current study we examined how factors such as felt emotion, motivation for sharing on social media as well as personality affected participants’ willingness to share self-reported emotion or facial expression online. By carrying out a GLMM analysis, this study found that participants’ willingness to share self-reported emotion and facial expressions was influenced by their personality traits and the motivation for sharing their emotion information that they were given. From our results we can conclude that the estimated level of privacy for certain emotional information, such as facial expression, is influenced by the motivation for sharing the information online.","tags":null,"title":"The effect of personality and social context on willingness to share emotion information on social media","type":"publication"},{"authors":["Damien Dupré","Michel Dubois","Anna Tcherkassof"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"6066820de43f1977f9eb1817c7d34a8f","permalink":"/publication/2017-acceptance-pto/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/publication/2017-acceptance-pto/","section":"publication","summary":"To anticipate a product commercial success, companies aim to ensure that product's characteristics are involving users and triggering a positive first user experience as well as all along product's life span. Among these characteristics, triggering users’ emotions is particularly relevant. This article aims to present the different approaches integrating the emotion generated by products. In order to illustrate the emotional potential of certain products a selection of 14 photographs of products was evaluated by 87 participants. Results reveal a significant correlation between cognitive, motivational and subjective components of emotions. A reflexion about the integration of cognitive appraisal assessment and motivational action readiness evaluation into a wilder emotional model is provided. Thus, emotions triggered by a product appear to be a stake for designers and an important variable to introduce in acceptability or user experience models.","tags":null,"title":"Role of emotions in product acceptance: Evaluation of the cognitive, motivational and subjective component","type":"publication"},{"authors":null,"categories":null,"content":"SYMPOSIUM 2: Affect Recognition in Humans versus Machines: Current Issues and Future Challenges Convener: Eva Krumhuber, University College London, UK\nABSTRACT: Automatic facial expression recognition systems can provide important information about our emotions and how they change over time. While the use of automatic systems has seen a steady increase over the last years, their classification results have not yet been systematically compared. The aim of this research was to test commercial software packages from Affectiva, Kairos and Microsoft companies in terms of their recognition accuracy. For this, we focused on spontaneous and dynamic facial expressions as provided by the DynEmo database –Disgust, Fear, Joy, and Surprise. In order to compare the classification results, we calculated the systems’ ratio of True positives (only the target label is recognised), False positives (the target label as well as a non-target label is recognised), True negatives (no label is recognised) and False negatives (target label is not recognised whereas a non-target label is). The results of the comparison between the systems showed comparable detection rates in term of True positives and False positives. However, their detection rates of False negatives and True negatives significantly differed between the different recognition systems. Specifically, systems were not equal in their tendency to detect non-target labels erroneously as well as in their tendency to not detect any emotion label. When examining emotion recognition accuracy for each video/emotion, videos with higher recognition accuracy were those that depicted as joyful facial expression. Other facial expressions resulted in a proportion of target emotion detection statistically equal or lower than the detection of non-target emotion. These results suggest that systems are not equivalent in their ability to detect specific spontaneous emotions. Therefore, users of such systems have to be aware of the strengths as well as of the potential limits of the data provided by automatic emotion recognition systems.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"829168eaa641e73affffe98045950239","permalink":"/talk/2018-cere/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2018-cere/","section":"talk","summary":"SYMPOSIUM 2: Affect Recognition in Humans versus Machines: Current Issues and Future Challenges Convener: Eva Krumhuber, University College London, UK\nABSTRACT: Automatic facial expression recognition systems can provide important information about our emotions and how they change over time. While the use of automatic systems has seen a steady increase over the last years, their classification results have not yet been systematically compared. The aim of this research was to test commercial software packages from Affectiva, Kairos and Microsoft companies in terms of their recognition accuracy.","tags":null,"title":"A Comparison of Three Commercial Systems for Automatic Recognition of Spontaneous Facial Expressions","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT: The analysis of facial expressions is currently a favoured method of inferring experienced emotion, and consequently significant efforts are currently being made to develop improved facial expression recognition techniques. Among these new techniques, those which allow the automatic recognition of facial expression appear to be most promising. This paper presents a new method of facial expression analysis with a focus on the continuous evolution of emotions using Generalized Additive Mixed Models (GAMM) and Significant Zero Crossing of the Derivatives (SiZer). The time-series analysis of the emotions experienced by participants watching a series of three different online videos suggests that analysis of facial expressions at the overall level may lead to misinterpretation of the emotional experience whereas non-linear analysis allows the significant expressive sequences to be identified.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"bb71d58a6ea9cc17ac083952cd366d61","permalink":"/talk/2018-bhci/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2018-bhci/","section":"talk","summary":"ABSTRACT: The analysis of facial expressions is currently a favoured method of inferring experienced emotion, and consequently significant efforts are currently being made to develop improved facial expression recognition techniques. Among these new techniques, those which allow the automatic recognition of facial expression appear to be most promising. This paper presents a new method of facial expression analysis with a focus on the continuous evolution of emotions using Generalized Additive Mixed Models (GAMM) and Significant Zero Crossing of the Derivatives (SiZer).","tags":null,"title":"Dynamic Analysis of Automatic Facial Expressions Recognition ‘in the Wild’ Using Generalized Additive Mixed Models and Significant Zero Crossing of the Derivatives","type":"talk"},{"authors":null,"categories":null,"content":"Embed your slides or video here using shortcodes. Further details can easily be added using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"cd6d9d084287506b4668ad90c6aff50a","permalink":"/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"talk","summary":"Embed your slides or video here using shortcodes. Further details can easily be added using Markdown and $\\rm \\LaTeX$ math code.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT: With the development of wearable sensors, it is now possible to assess the dynamic progression of physiological rhythms such as heart rate, breathing rate or galvanic skin response in ways and places and during certain activities that were previously not possible due to cost and reliability. This paper investigates the physiological changes when participating in a zipline activity. Despite the advances in sensor technology, the statistical analysis of such physiological signals remains a challenge for data analysts. This paper presents a workflow encompassing the whole process with the goal of obtaining a range of best fitting models to analyse the patterns given by these measurements “in the wild”.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"964a4c5a5dda77f0f2aaf0bdb0f7e5d1","permalink":"/talk/2018-mb/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2018-mb/","section":"talk","summary":"ABSTRACT: With the development of wearable sensors, it is now possible to assess the dynamic progression of physiological rhythms such as heart rate, breathing rate or galvanic skin response in ways and places and during certain activities that were previously not possible due to cost and reliability. This paper investigates the physiological changes when participating in a zipline activity. Despite the advances in sensor technology, the statistical analysis of such physiological signals remains a challenge for data analysts.","tags":null,"title":"Multivariate Body Area Network of Physiological Measures “In the Wild”: A case study with zipline activity","type":"talk"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461711600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461711600,"objectID":"80be3c9fcc86014efab0cec0f14957f6","permalink":"/project/deep-learning/","publishdate":"2016-04-27T00:00:00+01:00","relpermalink":"/project/deep-learning/","section":"project","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit.","tags":["Deep Learning"],"title":"Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":1461711600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461711600,"objectID":"553a94c5dfd3b8b099d8a12b2d248093","permalink":"/project/example-external-project/","publishdate":"2016-04-27T00:00:00+01:00","relpermalink":"/project/example-external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":["Damien Dupré","Daniel Akpan","Elena Elias","Jean-Michel Adam","Brigite Meillon","Nicolas Bonnefond","Michel Dubois","Anna Tcherkassof"],"categories":null,"content":"","date":1446336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446336000,"objectID":"f0684dc6daaf4418b49af034e176b749","permalink":"/publication/2015-oudjat-ijhcs/","publishdate":"2015-11-01T00:00:00Z","relpermalink":"/publication/2015-oudjat-ijhcs/","section":"publication","summary":"This paper describes Oudjat, a new software for the manual annotation of stimuli involved in facial expressions of emotion (FEE) recognition experiments. Taking into account advantages and weaknesses of existing software and keeping in mind the different specifications needed for annotation experiments, Oudjat provides a tradeoff between existing tools. For investigators, it is an easy-to-configure interface to set up relevant behaviors to be annotated. For annotators, it is an easy-to-use interface. This tool can perform complex annotations procedures with multiple responses panels such as buttons, scales as Likert scales, and free labeling. Oudjat also allows to chain response panels or to conduct sequence marking annotations (i.e. two-steps temporal annotation). As it can be configured in any language, Oudjat is particularly suited for intercultural experiments. Four annotation procedures are presented to illustrate Oudjat possibilities with FEE annotation. Oudjat is an open source software available to the scientific community and can be obtained on request.","tags":null,"title":"Oudjat, a configurable and usable annotation tool for the study of emotional stimuli","type":"publication"},{"authors":["Damien Dupré","Michel Dubois","Anna Tcherkassof","Pascal Pizelle"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"059c9c31e357ea6ed5170847ab6ab49f","permalink":"/publication/2015-acceptabilite-innovatio/","publishdate":"2015-09-01T00:00:00Z","relpermalink":"/publication/2015-acceptabilite-innovatio/","section":"publication","summary":"Anticipating the success of a product is a key issue for manufacturers, particularly in the case of an innovative product. To reduce the risk of being rejected by its target users, the field of psychology has examined the determinants of the emotional user experience generated by a product. The emotion felt by the user is indeed a key factor that will influence the use of a product. This paper presents the different methods for measuring emotions. A literature review will compare the studies measuring the emotions of potential product users, the type of measures they use and their main results. This literature review reveals that despite the importance of measuring the emotions of users to predict the success of products, few of them are performed on real products on the one hand and with a hypothetical-deductive approach on the other. Although the factor structure of emotional user experience is not yet explored, this framework is relevant for understanding the role of users’ emotions in their adoption of innovative products.","tags":null,"title":"Cadres et méthodes d’analyse des émotions suscitées par des produits innovants : Une revue bibliographique","type":"publication"},{"authors":["Anna Tcherkassof","Damien Dupré","Brigite Meillon","Nadine Mandran","Michel Dubois","Jean-Michel Adam"],"categories":null,"content":"","date":1383264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1383264000,"objectID":"5fe94f371cc15a1ea486a19f9c576749","permalink":"/publication/2013-dynemo-ijma/","publishdate":"2013-11-01T00:00:00Z","relpermalink":"/publication/2013-dynemo-ijma/","section":"publication","summary":"DynEmo is a database available to the scientific community (https://dynemo.upmf-grenoble.fr/). It contains dynamic and natural emotional facial expressions (EFEs) displaying subjective affective states rated by both the expresser and observers. Methodological and contextual information is provided for each expression. This multimodal corpus meets psychological, ethical, and technical criteria. It is quite large, containing two sets of 233 and 125 recordings of EFE of ordinary Caucasian people (ages 25 to 65, 182 females and 176 males) filmed in natural but standardized conditions. In the Set 1, EFE recordings are associated with the affective state of the expresser (self-reported after the emotion inducing task, using dimensional, action readiness, and emotional labels items). In the Set 2, EFE recordings are both associated with the affective state of the expresser and with the time line (continuous annotations) of observers' ratings of the emotions displayed throughout the recording. The time line allows any researcher interested in analysing non-verbal human behavior to segment the expressions into emotions.","tags":null,"title":"DynEmo: A video database of natural facial expressions of emotions","type":"publication"},{"authors":["Michel Dubois","Damien Dupré","Jean-Michel Adam","Anna Tcherkassof","Nadine Mandran","Brigite Meillon"],"categories":null,"content":"","date":1362096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1362096000,"objectID":"79e32558600c5d7635fb96d24cd7a99a","permalink":"/publication/2013-interface-jmui/","publishdate":"2013-03-01T00:00:00Z","relpermalink":"/publication/2013-interface-jmui/","section":"publication","summary":"The use of facial interfaces in distant communications highlights the relevance of emotional recognition. However researches on emotional facial expression (EFE) recognition are mainly based on static and posed stimuli and their results are not much transferable to daily interactions. The purpose of the present study is to compare emotional recognition of authentic EFEs with 11 different interface designs. A widget allowing participants both to recognize an emotion and to assess it on-line was used. Divided-face and compound-face interfaces are compared with a common full frontal interface. Analytic and descriptive on-line results reveal that some interfaces facilitate emotional recognition whereas others would decrease it. This study suggests that relevant interfaces could improve emotional recognition and thus facilitate distant communications.","tags":null,"title":"The influence of facial designs interfaces on dynamic emotional recognition","type":"publication"},{"authors":["Damien Dupré","Mathieu Loiseau","Hamad Salem","Philipe Dessus","Stephan Simonian"],"categories":null,"content":"","date":1351728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1351728000,"objectID":"c78336291ffe032fef2b23895aff8a3b","permalink":"/publication/2012-critere-re/","publishdate":"2012-11-01T00:00:00Z","relpermalink":"/publication/2012-critere-re/","section":"publication","summary":"","tags":null,"title":"Quelques critères d’utilisation d’un outil d’évaluation automatique de synthèses de cours à distance","type":"publication"},{"authors":["Philipe Dessus","S. Trausan-Matu","F. Wild","Damien Dupré","Mathieu Loiseau","T. Rebedea","V. Zampa"],"categories":null,"content":"","date":1320105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1320105600,"objectID":"cf72556bf7b02744c193325dd5cae0fe","permalink":"/publication/2011-environement-ds/","publishdate":"2011-11-01T00:00:00Z","relpermalink":"/publication/2011-environement-ds/","section":"publication","summary":"","tags":null,"title":"Un environnement personnel d'apprentissage évaluant des distances épistémiques et dialogiques","type":"publication"}]