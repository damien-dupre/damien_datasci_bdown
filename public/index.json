[{"authors":["admin"],"categories":null,"content":"Ph.D in Social and Experimental Psychology from the University Grenoble-Alpes, France, my researches are focusing on understanding psycho-physiological responses \u0026lsquo;in the wild\u0026rsquo;. With Anna Tcherkassof in Grenoble, I have contributed to develop the DynEmo database by recording and assessing dynamic and spontaneous facial expressions of emotions.\nWhereas my thesis aimed to evaluate Emotional User eXperience of innovative technologies and designs for the company Ixiade (www.ixiade.com), I have collaborated with Queen\u0026rsquo;s University and Sensum Ltd (www.sensum.co) to analyse emotions with physiological sensors and automatic facial expression recognition. I also worked at University College Dublin\u0026rsquo;s Insight Centre for Data Analytics in order to process and understand physiological measurements from marathon runners.\nNow Assistant Professor of Business Research Methods at Dublin City University, my domain of expertise relies in multivariate time series analysis and trend extraction for supervised or unsupervised machine learning classification.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Ph.D in Social and Experimental Psychology from the University Grenoble-Alpes, France, my researches are focusing on understanding psycho-physiological responses \u0026lsquo;in the wild\u0026rsquo;. With Anna Tcherkassof in Grenoble, I have contributed to develop the DynEmo database by recording and assessing dynamic and spontaneous facial expressions of emotions.","tags":null,"title":"Damien Dupré","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":[],"content":"\rLearning is changing quickly, very quickly. With the advancement of technologies and with the help of unexpected game changer events, the future of learning is becoming online rather than onsite. This is particularly true for learning how to code (here is a nice book about Education in Data Science with R) . Coding languages such as R or Python require a combination of contextual situations to be efficiently practiced:\nLearners need to see the output of running code. Classic slides lectures are good for general ideas but nothing can replace the direct feedback of run a code and comparing its output with expected results.\n\rLearners need to practice. Because the first will generally not produce the expected results, learners have to use the good old “die and retry” strategy to understand their errors. However it takes time and auto determination.\n\rLearners need to follow their own rhythm. Depending on learners’ previous experiences, the basics will be learnt faster or slower. All learners have their own pace.\n\r\rConsequently, online learning is perfect for these 3 points. In the R community, I imagine that most people have actually learnt R online, at least it is my case.\nLearning R online\rThere is a lot of amazing tutorials out there which can be accessed for free on coursera or udermy platforms for example or using blogs and online books. It is also very easy to create your own tutorial using {blogdown} or {bookdown} for example. A great talk entitled “How to Get Your Materials Online With R Markdown” recently introduced the different possibilities to share your own tutorial online (here are the video and the slides).\rHowever, few of these solutions are satisfying the first criteria. The ability to let the learners running the code by themselves usually imply that the learner has to run the code on a machine with the code and IDE required already installed.\nOne of these online learning platform is different, it allows the learner to watch a video or to read some slides and then to run the code via some exercises. I learnt a lot from this platform but the access of the tutorials is not free and therefore not suitable for students.\n\rInteractive tutorials R packages\rThe community around R has developed some amazing tools to create tutorials which allow learners to run their own code. There are maybe more of them but I am thinking of the {learnr} and {RTutor} packages (the {swirl} package also creates tutorials but these have to be used inside an IDE).\nThe {learnr} package is amazing. It helps to create very neat interactive tutorials from a .Rmd file. Because I love Rmarkdown, I have created some of them already for my students:\n\rWaRm Up!\rData Transformation\rData Visualisation\rLinear Regression\r\rIt is also possible to integrate the {learnr} package into a blogdown website and to create beautiful visualisation such as the giraffe project to learn statistics.\rI have never used the {RTutor} package but I’m assuming it has a lot of similarities with the {learnr} package as indicated in this blog post.\rThe {learnr} package is very nice but it has a major problem, the learners need to have their own and unique shiny session. To run a .Rmd file using the {learnr} package, this Rmarkdown has to be deployed on a shiny server and I currently know only two solutions to host a shiny app:\n1. With RStudio’s shinyapps.io\rThis option is super easy and fast to deploy a shiny app and has a free option to deploy 5 shiny apps (that I currently use for my {learnr} tutorials). However, this free option also only includes 5 users in the same time. This is great for the test of specific shiny apps but for parallel learning, it is definitely not enough.\nThere is the possibility to increase the number of parallel users but it is not free I’m afraid.\n\r2. With a cloud instance using shiny server\rThe second solution involves creating a cloud instance on AWS or DigitalOcean for example and installing the shiny server to host the Rmarkdown with {learnr}. This solution involves some patience, a bit of knowledge in shell coding and is basically not free (some providers give a free tiers but only for a limited period of time). About the scalability of these solutions, it is apparently possible to scale it to multiple learners in parallel, but I never used them for this purpose so I can’t tell more about how to do it and how many parallel users can use the shiny app.\nFrom what I know, there are two solutions to run a shiny app online which can scale up more than 5 users in the same time and unfortunately none of them are completely free (however I need to explore this AWS free tier solution).\nI’m sure that there are alternative solutions and hacks that I am not aware of. For example AWS has so many different services that are a mystery to me. Would it be possible to set a shiny app in an AWS ECS container or to create a lambda with a R environment? My knowledge in AWS competitors such as DigitalOcean, Microsoft Azure or Google Cloud is not good enough to suggest alternative free awesome solutions.\n\r\rScale Docker containers with Binder\rI recently came across the post of by Florencia d’Andrea explaining how to create an interactive tutorial with Binder, a free service which creates a scalable docker R environment attached to a github/gitlab repository. According to binder, the limit is 100 users simultaneously with is perfect for teaching!\nIn fact, I was already aware of online tutorial using this setup without knowing how they worked:\n\rGeneralized Additive Models in R by Noam Ross\rSupervised Machine Learning Case Studies in R by Julia Silge\rR-Bootcamp by Ted Laderas and Jessica Minnier\r\rBy following Florencia’s it is possible to create an interactive R tutorial for free very easily! The procedure described by Florencia and the three interactive tutorials examples as well as mine, are using the framework developed by Ines Montani called Juniper which will make the interface between the Binder docker R environment back end and the tutorial front end. Speaking of front end, Ines is providing a very fancy template using node.js and being deployed with Gatsby. At this point I’m throwing computer science jargon but my understanding of them is very limited. The most important is that it is working very well!\nIt’s not a perfect solution, see this blog post describing pros and cons for this solution, however it is working and for free.\n\rInteractive {blogdown} using ThebeLab’s Framework\rEven if Juniper’s framework is amazing, the content published is based on markdown files. I have nothing against markdown files but I do love Rmarkdown and I really missed them while publishing the tutorials. For this reason I googled “blogdown” + “binder” and found two very interesting discussions:\n\rThe first that I found was initiated here in a question about the interactivity of bookdown documents. Someone came across the activity of ThebeLab which is providing interfaces to binder for python code (see examples here and here. This question was answered by Christophe Dervieux saying that binder was not supporting R at this time. Christophe definitely made some progress when he shared in this tweet his success in creating an interactive .Rmd\rThe second discussion was on gitter.com initiated by Achintya Rao saying that he successfully managed to create an interactive blogdown.\r\rAfter being in contact with Achintya and by seeing Christophe’s code, it is possible to identify the essential component to make magic happened:\nThe blogdown theme or the .Rmd file need to include a code linking it to the repository connected to the binder docker R environment and to the package thebelab which is interacting between both.\r\rIn Christophe’s code, this html code is included directly in the .Rmd file:\n\u0026lt;!-- Some stuff from thebelab required for the magic --\u0026gt;\r\u0026lt;!-- Configure and load Thebe !--\u0026gt;\r\u0026lt;script type=\u0026quot;text/x-thebe-config\u0026quot;\u0026gt;\r{\rrequestKernel: true,\rbinderOptions: {\rrepo: \u0026#39;github_username/github_reponame,\r},\rkernelOptions: {\rname: \u0026quot;R\u0026quot;,\rkernelName: \u0026quot;ir\u0026quot;,\r},\r}\r\u0026lt;/script\u0026gt;\r\u0026lt;!-- script for thebelab --\u0026gt;\r\u0026lt;script src=\u0026quot;https://unpkg.com/thebelab@latest/lib/index.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;\rWhereas for Achintya’s blogdown, the following code is included in a specific file ./themes/hugo-lithium/layouts/partials/head_custom.html:\n\u0026lt;!-- Configure and load Thebe !--\u0026gt;\r\u0026lt;script type=\u0026quot;text/x-thebe-config\u0026quot;\u0026gt;\r{\rbootstrap: false,\rrequestKernel: true,\rbinderOptions: {\rrepo: \u0026#39;github_username/github_reponame,\rref: \u0026#39;master\u0026#39;,\rrepoProvider: \u0026#39;github\u0026#39;,\r},\rkernelOptions: {\rname: \u0026#39;ir\u0026#39;,\r}\r}\r\u0026lt;/script\u0026gt;\r\u0026lt;script src=\u0026quot;https://unpkg.com/thebelab@0.4.0/lib/index.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;\rBoth document are including a button to activate the code.\r\rIn Christophe’s code the button code and its action on the bootstrapThebe function are directly included in the Rmd file:\n# ad a button\rhtmltools::tags$button(\rid = \u0026quot;activateButton\u0026quot;,\rstyle=\u0026quot;width: 100px; height: 50px; font-size: 1.5em;\u0026quot;, \u0026quot;Activate\u0026quot;\r)\r# js chunk\r// thebelab js script\rvar bootstrapThebe = function() {\rthebelab.bootstrap();\r}\rdocument.querySelector(\u0026quot;#activateButton\u0026quot;).addEventListener(\u0026#39;click\u0026#39;, bootstrapThebe)\rFor Achintya’s blogdown, the code to create the button is included in a specific file ./themes/hugo-lithium/layouts/shortcodes/code.html and the code to trigger the bootstrapThebe is included in a specific file ./themes/hugo-lithium/layouts/partials/code.html. But to link these html files with the Rmd, each blogdown blog post have to include the following code:\nknitr::opts_chunk$set(collapse = TRUE, attr.source=\u0026quot;data-executable=\u0026#39;true\u0026#39;\u0026quot;,\reval = FALSE)\r\rAdding the link to the repository connected to binder\rI have hidden it to make it more generic and explicit but Christophe used the demo repo connected to binder https://github.com/binder-examples/r whereas Achintya has linked his blogdown to a repository hosted on his github.\nIn fact, the necessity of connecting a repository to binder and to indicate the link to this repository is also used in Juniper in a very similar way. However, instead of having a separated repository, Ines was connecting a branch of the same repository used for the deployment. From my github beginner experience, I think a distinct repository is easier to manage.\nThe necessity of having a distinct branch or repository is explained by Ines as follow in the readme of her template: “The reason they’re different is that binder forces a Docker container rebuild when a branch is updated. So, if we served our container out of master, it would rebuild every time we modified a chapter.md or an exercise.”\nTo build this binder docker image from a repository it is super easy, you just need two files on the root of the repository:\n\ra R script which contains all the package to install in order to run the code in your interactive chunks\ra text file containing YYYY-MM-DD snapshot at MRAN that will be used for installing libraries. See here for details.\rThe you just need to go to binder.org fill your repository url and branch (if not master) and this is it!\r\rYou can have a look at this blogdown test containing interactive code chunks here (this blogdown works in the same way as Achintya’s so but it is hosted on my github repository and uses a binder docker image connected to my github repository as well).\nAnd this his what Christope’s setup looks like:\r{\rrequestKernel: true,\rbinderOptions: {\rrepo: \"binder-examples/r\",\r},\rkernelOptions: {\rname: \"R\",\rkernelName: \"ir\",\r},\r}\r\r\r\rclicking the below button will also launch the kernel and that could take some\rtimes to strat. Be patient…\n\rActivate\r// thebelab js script\rvar bootstrapThebe = function() {\rthebelab.bootstrap();\r}\rdocument.querySelector(\"#activateButton\").addEventListener('click', bootstrapThebe)\r\rHere is a chunk, not evaluated on purpose by Rmarkdown, and to be executed interactively, directly in this document.\nprint(\u0026quot;Hello\u0026quot;)\r\rNext steps\rFor the moment, thanks to Achintya and Christophe’s work, I have a blog test will 2 code chunks that are interactive. The next move will be to convert my Juniper-mardown-node.js tutorial to a thebelab –rmarkdown-blogdown.\nAnother interesting part would be to integrate a code evaluation system like {gradethis} for example. I’m also wondering if {learnr} would work as well!\n\r","date":1586304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586364764,"objectID":"9be5a691b0b77ed7df521b57109d74ad","permalink":"/post/how-to-make-an-interactive-tutorial-to-teach-r-an-overview/","publishdate":"2020-04-08T00:00:00Z","relpermalink":"/post/how-to-make-an-interactive-tutorial-to-teach-r-an-overview/","section":"post","summary":"Learning is changing quickly, very quickly. With the advancement of technologies and with the help of unexpected game changer events, the future of learning is becoming online rather than onsite.","tags":[],"title":"How to make an interactive tutorial to teach R?","type":"post"},{"authors":[],"categories":[],"content":"\rI was recently asked to give a 3 sessions lecture about Data Analytics for Behavioural Research. As it is difficult to have a broader topic, I’ve decied to present my favorite research topic: the data science of emotions. These three different lecture present very quickly:\r1. the current theories of emotions\r2. how to use R\r3. how to use R for emotion data processing\nYou will find my slides and code on my github or hereafter. Enjoy!\n\r","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"e040acbb7ecd547e6f496813c3d0328e","permalink":"/post/my-data-analytics-for-behavioural-research-presentation-theory-introduction-to-r-and-data-processing/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/my-data-analytics-for-behavioural-research-presentation-theory-introduction-to-r-and-data-processing/","section":"post","summary":"I was recently asked to give a 3 sessions lecture about Data Analytics for Behavioural Research. As it is difficult to have a broader topic, I’ve decied to present my favorite research topic: the data science of emotions.","tags":[],"title":"My Data Analytics for Behavioural Research presentation: Theory, Introduction to R and Data Processing","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":[],"content":"\rPeople are saying that if you are repeating a code more than twice, then make a function for it. The idea of dplyrshortcut is that if you are using a function more than twice, then use a keyboard shortcut.\nInstallation\rYou can install the development version of dplyrshortcut with the devtools package.\n# install.packages(\u0026quot;devtools\u0026quot;)\rdevtools::install_github(\u0026quot;damien-dupre/dplyrshortcut\u0026quot;)\r\rUse\rdplyrshortcut add functions in RStudio IDE’s Addins section.\n\rThen, in order to add the shortcut to your keyboard, use the Rstudio IDE: Tools\u0026gt;Modify Keyboard Shortcuts... and associate the keyboard shortcut you prefer, such as:\n\rCtrl+Shift+F for “dplyr::filter(”\rCtrl+Shift+M for “dplyr::mutate(”\rCtrl+Shift+G for “dplyr::group_by(”\rCtrl+Shift+S for “dplyr::select(”\r\rand modify Insert Pipe Operator with Ctrl+Shift+P.\n\rAnd here you go! Data wrangling as fast as light speed!\n\r\r","date":1548460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548460800,"objectID":"27ebc64c4f95f0e92615fd194c126e57","permalink":"/post/dplyrshortcut-package-how-to-create-keyboard-shortcuts-for-dplyr-functions-on-rstudio-ide/","publishdate":"2019-01-26T00:00:00Z","relpermalink":"/post/dplyrshortcut-package-how-to-create-keyboard-shortcuts-for-dplyr-functions-on-rstudio-ide/","section":"post","summary":"People are saying that if you are repeating a code more than twice, then make a function for it. The idea of dplyrshortcut is that if you are using a function more than twice, then use a keyboard shortcut.","tags":[],"title":"`dplyrshortcut` package: How to create keyboard shortcuts for dplyr functions on RStudio IDE ","type":"post"},{"authors":null,"categories":[],"content":"\r\r\r\r\r\r\r\r\r\rOrganizing a wedding is … challenging but as Rusers we do have a major asset! One of the most challenging part is to find a venue. Indeed there are a lot of them but some are already booked for your date. So, in order to check if a venue is already booked I’ll show you how I made a list of possible venues with google search API, stored the list on google drive, web scrap for their emails and send them with R.\nSetup\r# store passwords\rlibrary(config)\r# data wrangling\rlibrary(plyr)\rlibrary(tidyverse)\rlibrary(purrr)\rlibrary(glue)\r# google APIs\rlibrary(googleway)\rlibrary(googledrive)\r# webscraping\rlibrary(rvest)\r# send emails\rlibrary(mailR)\rlibrary(XML)\rlibrary(RCurl)\r# html widgets\rlibrary(DT)\rlibrary(leaflet)\rknitr::opts_chunk$set(cache.extra = knitr::rand_seed, message = FALSE,warning = FALSE, error = FALSE)\rset.seed(123)# Seed for random number generation\roptions(scipen=999)# Disable scientific number format\rgmail_wedding \u0026lt;- config::get(\u0026quot;gmail_wedding\u0026quot;)\rgoogle_key \u0026lt;- config::get(\u0026quot;google_cloud\u0026quot;)\r\rList of Venues with google place API\rBecause almost everything is possible in R thanks to our awesome community, I was thinking of getting a list of venues from google. And thankfully there is a package to do that called googleway. Google API has many different services related to geocoding such as Direction API, Geolocalisation API or Place API which I’m using to get venues from search key words. To use it you just need to register on google cloud your Credit/Debit Card to obtain an API key, but no worries if you use the service in a gentle way, that won’t cost you anything. I found a very useful answer from stack overflow to use the googleway package.\nList of targeted cities\rI also don’t want to organize my wedding everywhere in France, I’d it to be in the Auvergne-Rhone-Alpes region which is a lovely area. So I wasn’t sure that by using the key word “Auvergne-Rhone-Alpes” I’ll find all the venues I wanted, so I decided to loop the search on a list of cities in this region based on their department numbers.\ndept_target \u0026lt;- c(01,07,26,38,69,73,74)\r#\rlist_city \u0026lt;- read.csv(\rfile = url(\u0026quot;https://sql.sh/ressources/sql-villes-france/villes_france.csv\u0026quot;),\rheader = FALSE) %\u0026gt;%\rdplyr::select(dept = V2, city = V5, pop2010 = V15) %\u0026gt;%\rdplyr::mutate(city = as.character(city)) %\u0026gt;%\rdplyr::filter(dept %in% dept_target) %\u0026gt;% # filter by target departments\rdplyr::filter(pop2010 \u0026gt; 5000) %\u0026gt;% # filter by city population size\rmagrittr::use_series(city)\r\rQuerying google place API\rOnce the list of cities obtained, I made a loop to query Google place for these cities. A tricky part is to get the search results from other pages. If a “next_page_token” is found, an while statement is querying for this new page. If no result is found the loop goes to the next city.\ndf_places_final \u0026lt;- NULL\rfor(city in list_city){\r#print(city)\rdf_places \u0026lt;- googleway::google_places(\rsearch_string = paste(\u0026quot;mariage\u0026quot;, city, \u0026quot;france\u0026quot;), key = google_key$key) # replace by your Google API key\rif(length(df_places$results) == 0) next\rdf_places_results \u0026lt;- df_places$results\rgeometry \u0026lt;- df_places_results$geometry$location\rdf_places_results \u0026lt;- df_places_results[,c(\u0026quot;name\u0026quot;,\u0026quot;formatted_address\u0026quot;,\u0026quot;place_id\u0026quot;,\u0026quot;types\u0026quot;)]\rdf_places_results \u0026lt;- cbind(df_places_results,geometry)\rwhile (!is.null(df_places$next_page_token)) {\rdf_places \u0026lt;- googleway::google_places(\rsearch_string = paste(\u0026quot;mariage\u0026quot;, city, \u0026quot;france\u0026quot;),\rpage_token = df_places$next_page_token,\rkey = google_key$key)\rdf_places_next \u0026lt;- df_places$results\rif(length(df_places_next)\u0026gt;0){\rgeometry \u0026lt;- df_places_next$geometry$location\rdf_places_next \u0026lt;- df_places_next[,c(\u0026quot;name\u0026quot;,\u0026quot;formatted_address\u0026quot;,\u0026quot;place_id\u0026quot;,\u0026quot;types\u0026quot;)]\rdf_places_next \u0026lt;- cbind(df_places_next,geometry)\rdf_places_results \u0026lt;- rbind(df_places_results, df_places_next)\r}\rSys.sleep(2) # time to not overload the google API\r}\rdf_places_final \u0026lt;- rbind(df_places_final,df_places_results)\r}\rObviously in the search results we obtain not only wedding venues but also photographers, caterers, and wedding shops. So an easy solution is to filter by name to find “castle”, “mansions” and “domains”. It should be noticed that there are some duplicated values to be filtered as well.\ndf_places_filtered \u0026lt;- df_places_final %\u0026gt;%\rdplyr::filter(grepl(\u0026#39;castle|chateau|domaine|manoir|ferme\u0026#39;, name,ignore.case = TRUE)) %\u0026gt;%\rdplyr::distinct(place_id, .keep_all = TRUE)\rWe can have an overview of the localisation of the venues thanks to the leaflet package.\nleaflet() %\u0026gt;%\raddTiles() %\u0026gt;% # Add default OpenStreetMap map tiles\raddMarkers(lng=df_places_filtered$lng, lat=df_places_filtered$lat, popup=df_places_filtered$name)\r\r{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"\u0026copy; OpenStreetMap contributors, CC-BY-SA\"}]},{\"method\":\"addMarkers\",\"args\":[[45.134445,44.8498099,44.7618727,44.693697,44.962252,44.513274,45.172233,45.3845806,45.2896691,45.272893,45.5686912,45.4290487,44.3599052,44.4000708,44.420168,44.2850276,44.341104,45.0212806,45.6375943,45.638319,45.6342439,43.9208334,45.2659883,45.640583,45.6690559,45.6701508,45.6368004,45.6053503,45.557085,45.5360143,43.8361643,45.211,45.8584009,45.9306015,45.9622121,45.967671,45.836602,45.8849645,46.0013656,45.922206,45.9586478,45.8928777,45.7676899,45.953119,45.7734206,45.3854787,45.564334,45.5356481,45.5539949,46.052035,46.01948,46.0770377,46.3110146],[4.990473,4.813384,4.88122,4.651336,4.780284,4.756764,4.7135882,4.9870776,4.9860176,4.434407,5.0570937,4.6808749,4.7100999,4.6801645,4.902971,4.5167063,4.766468,5.0762442,5.212144,5.035662,5.4337298,5.3090787,5.8762486,5.584276,5.4631044,5.4570628,5.3638768,5.8696337,5.2381668,6.1077085,3.9121832,5.660498,4.7400212,4.6878662,4.6699272,4.625038,4.597133,4.6144992,4.4976458,4.559744,4.6376306,4.4342168,5.0277671,4.7042232,4.7932634,5.955362,5.93751,5.9103388,5.9878989,6.330474,6.271978,6.5462569,6.479254],null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},[\"Domaine de Chantesse\",\"DOMAINE DE TURZON\",\"Le Domaine De Chanteperdrix\",\"Bijou Venues - Chateau du Bijou\",\"Chateau du Besset\",\"Manoir Le Roure\",\"Le Manoir de Munas\",\"Domaine de La Colombière\",\"Domaine de Clairbois\",\"Domaine de Duby\",\"Domaine de Grand Maison\",\"Domaine de la Griottiere\",\"Domaine Trusquin\",\"Domaine de Bel\",\"La Ferme Chapouton (hôtel, bistro gourmand, séminaire \u0026 mariages)\",\"Domaine du Clos d'Hullias\",\"Camping Saint Paul Trois Chateaux hill\",\"Domaine Des Seigneurs\",\"Domaine de Chanille : Salle Événementielle Réception Mariage Réunion Isère 38\",\"Chateau de Rajat\",\"Domaine de Suzel\",\"Domaine de Chantegrillet\",\"Domaine des Fontaines\",\"Domaine du Manoir\",\"Domaine de la Garenne\",\"Ferme de Montin location salle isère\",\"chateau teyssier de savy\",\"Castle Servolex\",\"Chateau de Cesarges\",\"DOMAINE DU CHÂTEAU DE LA RIVE\",\"Domaine des Rives\",\"Castle Sassenage\",\"Domaine des Calles\",\"Domaine de Bellevue\",\"Domaine Passeloup\",\"Manoir de la Garde\",\"Manoir Tourieux\",\"Castle Courbeville\",\"Domaine de Montfriol\",\"Domaine de la Genetière\",\"Domaine Des Coteaux D'or\",\"Un Manoir à Tarare\",\"Domaine Fantasia\",\"Chateau de Saint Trys\",\"domaine de chanille siege\",\"Castle of Montalieu\",\"Chateau de Boigne\",\"Domaine des Saints Pères\",\"Chateau mariage\",\"Castle of Saint-Sixt\",\"Domaine de la Sapinière\",\"Hôtel La Ferme Du Lac - Restaurant Au vieux Chalet\",\"La Ferme du Chateau\"],null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"limits\":{\"lat\":[43.8361643,46.3110146],\"lng\":[3.9121832,6.5462569]}},\"evals\":[],\"jsHooks\":[]}\r\rObtaining venues’ website\rgoogleway::google_place() is great to find places with their addresses, GPS coordinates and types but the first loop doesn’t provide their website URLs. In order to get them we have to query the Google place API using venues “places_id” with googleway::google_place_details() by applying a small function to with purrr.\nget_website \u0026lt;- function(place_id){\r#print(place_id)\rplace_id \u0026lt;- as.character(place_id)\rdat \u0026lt;- googleway::google_place_details(place_id = place_id, key = google_key$key)\rres \u0026lt;- ifelse(is.null(dat$result$website),\u0026quot;no_website\u0026quot;,dat$result$website)\rreturn(res)\r}\rwebsite_list \u0026lt;- df_places_filtered$place_id %\u0026gt;%\rpurrr::map(get_website) %\u0026gt;%\runlist()\rdf_places_filtered$website \u0026lt;- website_list\rFor the next stages of the process I’m going to remove the venues without website URL but if like me you are organizing your wedding I suggest to manually contact them. Most of the URL are clean but sometimes some errors are possible so it is possible to remove them with a gsub() for example. I’m creating a new variable called “website_contact” which will be used as well for web scraping.\ndf_places_filtered \u0026lt;- df_places_filtered %\u0026gt;%\rdplyr::filter(website != \u0026quot;no_website\u0026quot;) %\u0026gt;%\rdplyr::mutate(website = gsub(\u0026quot;\\\\,.*\u0026quot;,\u0026quot;\u0026quot;,website)) %\u0026gt;%\rdplyr::mutate(website = gsub(\u0026quot;com/fr\u0026quot;,\u0026quot;com\u0026quot;,website)) %\u0026gt;%\rdplyr::mutate(website_contact = paste0(website,\u0026quot;contact\u0026quot;))\rThe list of venues is now “clean” we can start the web scraping to obtain venues’ emails.\n\r\rWebsite scraping for emails\rI already said that Google place is great but as far as I know it doesn’ provides venues emails. However we won’t stop here and R is providing some excellent tool like rvest package in order to get information for the web. Thankfully, venues websites made their emails very easy to get on their home page or on their contact page so the idea is to web scrap these pages to see if we can find venues emails in a very short function. The function contains trycatch to check the URL before scraping for emails.\nextract_email \u0026lt;- function(website){\r#print(website)\rurl_test \u0026lt;- tryCatch(xml2::read_html(website), error=function(e) print(\u0026quot;url_error\u0026quot;))\rif(url_test == \u0026quot;url_error\u0026quot;){\rreturn(NA)\r} else {\rtext_web \u0026lt;- xml2::read_html(website)%\u0026gt;%\rrvest::html_text()\remail_text \u0026lt;- unlist(regmatches(text_web, gregexpr(\u0026quot;([_a-z0-9-]+(\\\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\\\.[a-z0-9-]+)*(\\\\.[a-z]{2,4}))\u0026quot;, text_web)))\remail_text \u0026lt;- gsub(\u0026quot;\\n\u0026quot;,\u0026quot;\u0026quot;,email_text)\remail_text \u0026lt;- gsub(\u0026quot; \u0026quot;,\u0026quot;\u0026quot;,email_text)\rreturn(email_text[1])\r}\r}\r# web scraping home page\remail_list \u0026lt;- df_places_filtered$website %\u0026gt;%\rpurrr::map(extract_email) %\u0026gt;%\runlist()\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\rdf_places_filtered$email \u0026lt;- email_list\r# web scraping contact page\remail_list \u0026lt;- df_places_filtered$website_contact %\u0026gt;%\rpurrr::map(extract_email) %\u0026gt;%\runlist()\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\r## [1] \u0026quot;url_error\u0026quot;\rdf_places_filtered$email_contact \u0026lt;- email_list\r# merge email and email_contact\rdf_places_filtered \u0026lt;- df_places_filtered %\u0026gt;%\rdplyr::mutate(email = ifelse(is.na(email),email_contact,email)) %\u0026gt;%\rdplyr::filter(!is.na(email)) %\u0026gt;%\rdplyr::select(-email_contact, -types)\rdf_places_filtered %\u0026gt;%\rdplyr::select(name, website) %\u0026gt;%\rDT::datatable(options = list(pageLength = 5))\r\r{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\"],[\"DOMAINE DE TURZON\",\"Le Domaine De Chanteperdrix\",\"Manoir Le Roure\",\"Le Manoir de Munas\",\"Domaine de La Colombière\",\"Domaine de Duby\",\"Domaine de Grand Maison\",\"Domaine de la Griottiere\",\"Domaine Trusquin\",\"La Ferme Chapouton (hÃ´tel, bistro gourmand, sÃ©minaire \u0026amp; mariages)\",\"Domaine du Clos d'Hullias\",\"Camping Saint Paul Trois Chateaux hill\",\"Domaine Des Seigneurs\",\"Domaine de Chanille : Salle Événementielle Réception Mariage Réunion Isère 38\",\"Chateau de Rajat\",\"Domaine des Fontaines\",\"Domaine de la Garenne\",\"Ferme de Montin location salle isère\",\"chateau teyssier de savy\",\"Castle Servolex\",\"DOMAINE DU CHÂTEAU DE LA RIVE\",\"Manoir de la Garde\",\"Manoir Tourieux\",\"Castle Courbeville\",\"Domaine de Montfriol\",\"Domaine de la Genetière\",\"Un Manoir à Tarare\",\"Domaine Fantasia\",\"Castle of Saint-Sixt\",\"Domaine de la Sapinière\"],[\"http://www.domainedeturzon.com/\",\"http://www.domainedechanteperdrix.fr/\",\"http://www.domaine-le-roure.com/\",\"http://www.lemanoirdemunas.fr/\",\"https://www.lacolombiere.com/\",\"http://domainededuby.fr/\",\"http://www.reception-grandmaison-seminaire-vienne-lyon.fr/\",\"http://www.domainedelagriottiere.com/\",\"http://www.domainedutrusquin.fr/\",\"https://chapouton.com/\",\"http://domaine-mariage-avignon-provence.com/\",\"http://campingdelacolline.com/\",\"http://www.domainedesseigneurs.fr/\",\"http://www.domaine-de-chanille.com/\",\"http://www.chateaurajat.fr/\",\"http://www.domaine-des-fontaines.com\",\"https://salle-de-mariage-isere.com/\",\"http://www.fermedemontin.com/\",\"http://www.chateau-teyssier-de-savy.fr/contact.html\",\"http://www.chateau-servolex.com/\",\"http://www.chateaudelarive.com/\",\"http://www.manoirdelagarde.com/\",\"http://www.manoirtourieux.com/\",\"http://www.chateaudecourbeville.fr/\",\"http://www.domainemontfriol.sitew.fr/\",\"http://www.genetiere.fr/\",\"https://manoirtarare.fr/\",\"http://www.fantasiareception.fr/\",\"http://www.chateaudesaintsixt.com/\",\"http://www.domaine-sapiniere.com/?utm_medium=organic\u0026amp;utm_source=google\u0026amp;utm_campaign=google_my_business\"]],\"container\":\"\\n \\n \\n  \\n name\\n website\\n \\n \\n\",\"options\":{\"pageLength\":5,\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"columnDefs\":[{\"orderable\":false,\"targets\":0}],\"lengthMenu\":[5,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}\rExcellent we obtained a list of some venues to email!\n\rgoogle drive and automatic emails\rA classic advice for wedding planning is to setup a specific email only dedicated to this task. One advantage of Google is not only very easy email setup but also the access to a Google drive to store your documents in order to keep track. Of course it’s possible to store it locally but I found Google drive nice for sharing with your partner.\nOnce it’s done, Google drive documents are easily accessed with the package googledrive (see https://googledrive.tidyverse.org/index.html for some information about googledrive).\nUpload list of venues to google drive\rthe workflow of googledrive is quite specific, we must save the data frame locally first and then upload the file.\n# first save the list of venues local\rwrite.csv(df_places_filtered, \u0026quot;list_venues.csv\u0026quot;,row.names = FALSE)\r# upload to google drive\rdrive_upload(media = \u0026quot;list_venues.csv\u0026quot;, name = \u0026quot;list_venues\u0026quot;,type = \u0026quot;spreadsheet\u0026quot;)\r\rDownload list of venues from google drive\rOnce it is done, we have to download the file locally and to read it again.\n# select file id from google drive\rlist_venues_id \u0026lt;- drive_find() %\u0026gt;%\rdplyr::filter(name == \u0026quot;list_venues\u0026quot;) %\u0026gt;%\rmagrittr::use_series(id)\r# download list of venues locally\rdrive_download(as_id(list_venues_id),overwrite = TRUE, type = \u0026quot;csv\u0026quot;)\r# read local list of venues file\rlist_venues \u0026lt;- read.csv(\u0026quot;list_venues.csv\u0026quot;,row.names = NULL) %\u0026gt;%\rdplyr::mutate_if(is.factor,as.character)\r\rSelect email to be send\rNow the list of venues is stored in Google drive it’s time to send our emails with R. Because it is easier for me, I’ve set up another for loop (yes it’s not great but very re insuring at least). For each row of the data frame we are going to extract the venue name and email and send the same text ask for availability at a certain date.\nAn important thing to be able to send emails from R is to allow less secure app: Yes in gmail settings.\nemail_to_send \u0026lt;- list_venues\r#\r# Email to send\remail_text \u0026lt;- \u0026quot;\u0026lt;p\u0026gt;Dear owner/manager of \u0026#39;{name}\u0026#39;, \u0026lt;br\u0026gt;\u0026lt;br\u0026gt;We are contacting you because we would like to organise our wedding \u0026lt;b\u0026gt;Sunday 9 of June 2019\u0026lt;/b\u0026gt; and your plac would be amazing for it.\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;That\u0026#39;s why we would like to know if your venue \u0026#39;{name}\u0026#39; is available \u0026lt;b\u0026gt;Sunday 9 of June 2019\u0026lt;/b\u0026gt;?\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;Best regards,\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;YOUR NAMES\u0026lt;/p\u0026gt;\u0026quot;\r#\rfor(i in 1:nrow(email_to_send)){\rdf \u0026lt;- email_to_send[i,]\rname \u0026lt;- as.character(df$name)\r################################\rsend.mail(from = gmail_wedding$email,\rto = as.character(df$email),\rsubject = \u0026quot;Availability for a wedding on the 09/06/2019\u0026quot;,\rbody = glue::glue(email_text),\rsmtp = list(host.name = \u0026quot;smtp.gmail.com\u0026quot;, port = 465, user.name = gmail_wedding$email, passwd = gmail_wedding$passwd, ssl = TRUE),\rauthenticate = TRUE,\rsend = TRUE,\rhtml = TRUE)\r}\rYou can have a look at the email send in your mail box in order to check that the process worked.\nThen, the final stage it to update the list of venue with the contact date in order to not send an email twice.\nemail_to_send \u0026lt;- email_to_send %\u0026gt;%\rdplyr::mutate(date_contact = as.character(as.Date(Sys.Date()))) %\u0026gt;%\rdplyr::mutate(type_contact = \u0026quot;automatic email\u0026quot;)\r# Checks in case of different batch of email sending\rid \u0026lt;- match(list_venues$name, email_to_send$name, nomatch = 0L)\rlist_venues$date_contact[id != 0] \u0026lt;- email_to_send$date_contact[id]\rlist_venues$type_contact[id != 0] \u0026lt;- email_to_send$type_contact[id]\r# Write data on local and Upload data from local to google drive\rwrite.csv(list_venues,\u0026quot;ist_venues.csv\u0026quot;,row.names = FALSE)\rdrive_update(file = \u0026quot;list_venues\u0026quot;, media = \u0026quot;list_venues.csv\u0026quot;)\rI hope these scripts will help you in finding the best place for your wedding. And good luck for the organisation!\n\r\r","date":1542067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542067200,"objectID":"cfa1aedc9aabf46d1c729365e39d5ca7","permalink":"/post/how-to-organise-a-wedding-with-r-google-search-api-google-drive-web-scraping-and-automatic-emails/","publishdate":"2018-11-13T00:00:00Z","relpermalink":"/post/how-to-organise-a-wedding-with-r-google-search-api-google-drive-web-scraping-and-automatic-emails/","section":"post","summary":"Organizing a wedding is … challenging but as Rusers we do have a major asset! One of the most challenging part is to find a venue.","tags":[],"title":"How to organize a wedding with R: google place API, google drive, web scraping and automatic emails","type":"post"},{"authors":null,"categories":[],"content":"\rLike every good movies, my previous blog post “Time series clustering with Dynamic Time Warping” deserves a sequel. In this Part 2, I will have a look at the athletes’ training plan for a marathon. Because marathons are such a demanding performance, most of athletes have a specific training plan to follow in order to be prepared. Many different training plan can be found on the web such as this one from the website www.runireland.com.\nIn this blog post I will try to cluster different simulated athlete training plans with Dynamic Time Warping and some seasonality, states and power band extractions.\nList of packages needed\r# data wrangling\rlibrary(dplyr) # data wrangling\rlibrary(tidyr) # datawrangling\r# analysis\rlibrary(dtwclust) # dynamic time warpping\rlibrary(depmixS4) # Hidden Markov Model\rlibrary(WaveletComp) # Wavelet Analysis\r# graphics\rlibrary(ggplot2) # grammar of graphics\rlibrary(ggdendro) # grammar of dendrograms\rlibrary(gtable) # plot organisation\rlibrary(grid) # plot organisation\rlibrary(gridExtra) # plot organisation\r\rData simulation\rFor this purpose, I will create a data frame of 20 athlete training plans with 10 of them with a random plan and 10 with a repeated pattern non synchronized on their date and intensity. The main variable is the distance they have ran of every day since 25 weeks (175 days) before the marathon.\ndate_marathon \u0026lt;- as.Date(\u0026quot;2015-10-26\u0026quot;)\r#\rdf \u0026lt;- NULL\r# random training plan with runs from 5 to 40km with a high proability of non run days (between 25% and 75% depending on athletes)\rfor (i in 1:10){\rrandom_proba \u0026lt;- runif(8)\rrandom_proba \u0026lt;- random_proba/sum(random_proba)\rvalue \u0026lt;- base::sample(\rx = seq(from = 0, to = 40, by = 5), size = 175, replace = TRUE, prob = c(runif(1, 0.25, 0.75),random_proba)\r)\rathlete \u0026lt;- paste0(\u0026quot;athlete_rand_\u0026quot;,i)\rnew_df \u0026lt;- data.frame(athlete = athlete, value = value, rundate = seq.Date(date_marathon-175, date_marathon-1, by=\u0026quot;day\u0026quot;))\rdf \u0026lt;- rbind(df,new_df)\r}\r# training plan with a reapeated pattern with can change according the weeks and with a different intensity according athletes\rfor (i in 11:20){\rvalue \u0026lt;- rep_len(\rx = c(rep(x = 0, sample(1:3, 1)),10,0,15,20,30)*runif(1, 0.5, 1.5),\rlength.out = 175\r)\rathlete \u0026lt;- paste0(\u0026quot;athlete_plan_\u0026quot;,i)\rnew_df \u0026lt;- data.frame(athlete = athlete, value = value, rundate = seq.Date(date_marathon-175, date_marathon-1, by=\u0026quot;day\u0026quot;))\rdf \u0026lt;- rbind(df,new_df)\r}\rOnce we have the data generated, a key trick will be to convert this data frame into a list of time series. The reason behind this choice is the possibility to implement a multivariate DTW analysis in the future (maybe in a Part 3).\nplan_list \u0026lt;- df %\u0026gt;% tidyr::spread(athlete,value) %\u0026gt;%\rdplyr::select(-rundate) %\u0026gt;%\rpurrr::map(~(.))\r\rDTW cluster on raw data\rAfter creating the list of data, let’s implement a simple DTW clustering on the raw data to see if we can identify our two groups.\nDTW model\rNclust \u0026lt;- 2\rdtw_model \u0026lt;- dtwclust::tsclust(series = plan_list, type = \u0026quot;h\u0026quot;, k = Nclust, distance = \u0026quot;dtw_basic\u0026quot;, control = hierarchical_control(method = \u0026quot;complete\u0026quot;),\rpreproc = NULL, #args = tsclust_args(dist = list(window.size = 5L)),\rtrace = TRUE)\r#\rdtw_data \u0026lt;- ggdendro::dendro_data(dtw_model, type=\u0026quot;rectangle\u0026quot;)\r#\rlabels_order \u0026lt;- dtw_data$labels$label\r#\rdtw_result \u0026lt;- data.frame(label = names(plan_list), cluster = factor(stats::cutree(dtw_model, k = Nclust)))\r#\rdtw_data[[\u0026quot;labels\u0026quot;]] \u0026lt;- merge(dtw_data[[\u0026quot;labels\u0026quot;]], dtw_result, by=\u0026quot;label\u0026quot;)\rdtw_result \u0026lt;- dplyr::full_join(dtw_result,dtw_data$labels, by=c(\u0026quot;label\u0026quot;, \u0026quot;cluster\u0026quot;))%\u0026gt;%\rdplyr::arrange(x)\r\rDTW plot\rcluster_box \u0026lt;- aggregate(x~cluster, ggdendro::label(dtw_data), range)\rcluster_box \u0026lt;- data.frame(cluster_box$cluster,cluster_box$x)\rcluster_threshold \u0026lt;- mean(dtw_model$height[length(dtw_model$height)-((Nclust-2):(Nclust-1))])\r#\rnumColors \u0026lt;- length(levels(dtw_result$cluster)) # How many colors you need\rgetColors \u0026lt;- scales::hue_pal() # Create a function that takes a number and returns a qualitative palette of that length (from the scales package)\rmyPalette \u0026lt;- getColors(numColors)\rnames(myPalette) \u0026lt;- levels(dtw_result$cluster) # Give every color an appropriate name\rp1 \u0026lt;- ggplot() + geom_rect(data=cluster_box, aes(xmin=X1-.3, xmax=X2+.3, ymin=0, ymax=cluster_threshold, color=cluster_box.cluster), fill=NA)+\rgeom_segment(data=ggdendro::segment(dtw_data), aes(x=x, y=y, xend=xend, yend=yend)) + coord_flip() + scale_y_continuous(\u0026quot;Distance\u0026quot;) + scale_x_continuous(\u0026quot;\u0026quot;,breaks = 1:20, labels = labels_order) + guides(color=FALSE, fill = FALSE)+\rtheme(\rpanel.grid.major = element_blank(), panel.grid.minor = element_blank(), # remove grids\rpanel.background = element_blank(), axis.text.y = element_text(colour = myPalette[dtw_result$cluster],hjust=0.5),\raxis.ticks.y=element_blank()\r)\r#\rp2 \u0026lt;- as.data.frame(matrix(unlist(plan_list), nrow=length(unlist(plan_list[1])), dimnames = list(c(),names(plan_list)))) %\u0026gt;%\rdplyr::mutate(rundatelocal = seq.Date(date_marathon-175, date_marathon-1, by=\u0026quot;day\u0026quot;)) %\u0026gt;%\rtidyr::gather(key = label,value = value, -rundatelocal) %\u0026gt;%\rdplyr::mutate(label = as.factor(label)) %\u0026gt;%\rdplyr::full_join(., dtw_result, by = \u0026quot;label\u0026quot;) %\u0026gt;% mutate(label = factor(label, levels = rev(as.character(labels_order)))) %\u0026gt;%\rggplot(aes(x = rundatelocal, y = value, colour = as.factor(cluster))) +\rgeom_line() +\rgeom_area(aes(fill = as.factor(cluster))) +\rcoord_cartesian(ylim = c(0,50)) +\rscale_y_continuous(name = \u0026quot;Total distance per day [km]\u0026quot;, breaks = seq(0, 50, by = 50)) +\rscale_x_date(name = \u0026quot;Run Date\u0026quot;, date_breaks = \u0026quot;4 week\u0026quot;, date_labels = \u0026quot;%b %d\u0026quot;) +\rfacet_wrap(~label, ncol = 1, strip.position=\u0026quot;left\u0026quot;) + guides(color=FALSE, fill = FALSE) +\rtheme_bw() + theme(strip.background = element_blank(), strip.text = element_blank())\r#\rplt_list \u0026lt;- list(p2, p1)\rplt_layout \u0026lt;- rbind(c(NA, 2),\rc(1, 2),\rc(NA, 2))\r#\rgrid.arrange(grobs = plt_list, layout_matrix = plt_layout, heights = c(0.04, 1, 0.05))\rI think I get a nice plot thanks to the solutions provided on Stack Overflow graphically speaking (except some overlap with the labels but I’m working on it). The results are not too bad but some of the random plan can be included in the repeated pattern plan. Well randomness can be expected and can create some nice patterns sometimes. Another interesting result is the necessity to increase the cluster number in order to have a clean clustering.\n\rCentroids\rWe can also have a look at the centroids to see with plans are the most representative of the clusters. Obviously with only two clusters, it is not very useful but it can be a key element to distinguish between many different training plans.\ndtw_model_centroids \u0026lt;- data.frame(dtw_model@centroids, rundatelocal = seq.Date(date_marathon-175, date_marathon-1, by=\u0026quot;day\u0026quot;)) %\u0026gt;%\rtidyr::gather(label, totaldistancekm, starts_with(\u0026quot;athlete\u0026quot;)) %\u0026gt;%\rdplyr::left_join(., dtw_result, by = \u0026quot;label\u0026quot;) %\u0026gt;% dplyr::mutate(label = factor(label, levels = rev(labels_order)))\r#\rdtw_model_centroids %\u0026gt;%\rggplot(aes(rundatelocal,totaldistancekm, color = cluster, fill = cluster)) +\rgeom_line() +\rgeom_area() +\rfacet_wrap(~ label + cluster, ncol = 1, strip.position=\u0026quot;right\u0026quot;, labeller=labeller(.rows = label_both)) +\rscale_y_continuous(name = \u0026quot;Total distance per day [km]\u0026quot;) +\rscale_x_date(name = \u0026quot;Run Date\u0026quot;, date_breaks = \u0026quot;4 week\u0026quot;, date_labels = \u0026quot;%b %d\u0026quot;) +\rguides(color=FALSE, fill = FALSE) +\rtheme_bw()\rThe main problem with raw data is the noise. In order to extract recurrent patterns, the randomness of the noise can sometimes simulate non meaningful pattern and then change the cluster structure. Because we are interested in classification of recurrent pattern, a nice thing would be to remove the noise. Noise removal analyses are probably the most important contribution of signal treatment research and many can be applied here such as seasonality decomposition, Hidden Markov Models and power spectrum analysis.\n\rDTW cluster with seasonality decomposition\rR for time series analysis have some unavoidable packages and functions. If you are interested in time series analysis, you probably cannot work without zoo::zoo(), xts::xts() or tibbletime::as_tbl_time(). However for time series analysis, the stats package have one of the most used and nice function: stl(). Stl allows a Seasonal Decomposition of Time Series by Loess which is powerful in order to extract time series noise, trend and seasonality (i.e periods). In our case we will try to use stl() in order to extract training plan seasonality over one week and then to cluster the results with the DTW method.\nSo first let’s apply the stl() decomposition to every time series in our master list.\nextract_seasonality \u0026lt;- function(x, robust){\rx_ts = ts(as.numeric(unlist(x)), frequency = 7)\rstl_test = stl(x_ts, s.window = 7, robust)\rreturn(stl_test$time.series[,1])\r}\r#\rplan_seasonality \u0026lt;- plan_list %\u0026gt;%\rpurrr::map(~extract_seasonality(., robust = TRUE))\rand then let’s process our model and to plot the results.\nNclust \u0026lt;- 2\rdtw_model \u0026lt;- dtwclust::tsclust(series = plan_seasonality, type = \u0026quot;h\u0026quot;, k = Nclust, distance = \u0026quot;dtw_basic\u0026quot;, control = hierarchical_control(method = \u0026quot;complete\u0026quot;),\rpreproc = NULL, #args = tsclust_args(dist = list(window.size = 5L)),\rtrace = TRUE)\r#\rdtw_data \u0026lt;- ggdendro::dendro_data(dtw_model, type=\u0026quot;rectangle\u0026quot;)\r#\rlabels_order \u0026lt;- dtw_data$labels$label\r#\rdtw_result \u0026lt;- data.frame(label = names(plan_seasonality), cluster = factor(stats::cutree(dtw_model, k = Nclust)))\r#\rdtw_data[[\u0026quot;labels\u0026quot;]] \u0026lt;- merge(dtw_data[[\u0026quot;labels\u0026quot;]], dtw_result, by=\u0026quot;label\u0026quot;)\rdtw_result \u0026lt;- dplyr::full_join(dtw_result,dtw_data$labels, by=c(\u0026quot;label\u0026quot;, \u0026quot;cluster\u0026quot;))%\u0026gt;%\rdplyr::arrange(x)\rcluster_box \u0026lt;- aggregate(x~cluster, ggdendro::label(dtw_data), range)\rcluster_box \u0026lt;- data.frame(cluster_box$cluster,cluster_box$x)\rcluster_threshold \u0026lt;- mean(dtw_model$height[length(dtw_model$height)-((Nclust-2):(Nclust-1))])\r#\rnumColors \u0026lt;- length(levels(dtw_result$cluster)) # How many colors you need\rgetColors \u0026lt;- scales::hue_pal() # Create a function that takes a number and returns a qualitative palette of that length (from the scales package)\rmyPalette \u0026lt;- getColors(numColors)\rnames(myPalette) \u0026lt;- levels(dtw_result$cluster) # Give every color an appropriate name\rp1 \u0026lt;- ggplot() + geom_rect(data=cluster_box, aes(xmin=X1-.3, xmax=X2+.3, ymin=0, ymax=cluster_threshold, color=cluster_box.cluster), fill=NA)+\rgeom_segment(data=ggdendro::segment(dtw_data), aes(x=x, y=y, xend=xend, yend=yend)) + coord_flip() + scale_y_continuous(\u0026quot;Distance\u0026quot;) + scale_x_continuous(\u0026quot;\u0026quot;,breaks = 1:20, labels = labels_order) + guides(color=FALSE, fill = FALSE)+\rtheme(\rpanel.grid.major = element_blank(), panel.grid.minor = element_blank(), # remove grids\rpanel.background = element_blank(), axis.text.y = element_text(colour = myPalette[dtw_result$cluster],hjust=0.5),\raxis.ticks.y=element_blank()\r)\r#\rp2 \u0026lt;- as.data.frame(matrix(unlist(plan_seasonality), nrow=length(unlist(plan_seasonality[1])), dimnames = list(c(),names(plan_seasonality)))) %\u0026gt;%\rdplyr::mutate(rundatelocal = seq.Date(date_marathon-175, date_marathon-1, by=\u0026quot;day\u0026quot;)) %\u0026gt;%\rtidyr::gather(key = label,value = value, -rundatelocal) %\u0026gt;%\rdplyr::mutate(label = as.factor(label)) %\u0026gt;%\rdplyr::full_join(., dtw_result, by = \u0026quot;label\u0026quot;) %\u0026gt;% mutate(label = factor(label, levels = rev(as.character(labels_order)))) %\u0026gt;%\rggplot(aes(x = rundatelocal, y = value, colour = as.factor(cluster))) +\rgeom_line() +\rgeom_area(aes(fill = as.factor(cluster))) +\rcoord_cartesian(ylim = c(-25,25)) +\rscale_y_continuous(name = \u0026quot;Seasonal distance per day [km]\u0026quot;, breaks = seq(-25, 25, by = 50)) +\rscale_x_date(name = \u0026quot;Run Date\u0026quot;, date_breaks = \u0026quot;4 week\u0026quot;, date_labels = \u0026quot;%b %d\u0026quot;) +\rfacet_wrap(~label, ncol = 1, strip.position=\u0026quot;left\u0026quot;) + guides(color=FALSE, fill = FALSE) +\rtheme_bw() + theme(strip.background = element_blank(), strip.text = element_blank())\r#\rplt_list \u0026lt;- list(p2, p1)\rplt_layout \u0026lt;- rbind(c(NA, 2),\rc(1, 2),\rc(NA, 2))\r#\rgrid.arrange(grobs = plt_list, layout_matrix = plt_layout, heights = c(0.04, 1, 0.05))\rWell that’s an epic fail I think but let’s have a look why. Different reasons can explain why we have a first cluster with only 3 time series and as second with all the 17 remaining ones:\nI am using only 2 clusters. In the real life (and in real randomness) a large amount of pattern is possible thus increasing the number of clusters can make the clustering more efficient (if an evaluation of the best fit with cluster number is performed).\rBy removing the noise in the random plan, I made them not random at all and we can see now the repetition of the patterns. This is exactly what I want in my research with real data but here it made a mess.\r\rSo let’s try another method!\n\rDTW cluster with Hidden Markov Model\rI’m not a perfect expert in Hidden Markov Model (HMM) and after having a look at the book Hidden Markov Models for Time Series An Introduction Using R by Walter Zucchini, Iain L. MacDonald and Roland Langrock, I can surely say that it is a complicated question. However in a nutshell HMM are clustering the values according their probability to be part of a “state”. In our case, let’s say that we have three states possible per day “no run”, “medium run” and “long run”. By using HMM it is possible to create new time series based on states instead on distance. It’s a qualitative transformation without any prior assumption about the states boundaries (almost).\nplan_HMM \u0026lt;- as.data.frame(matrix(unlist(plan_list), nrow=length(unlist(plan_list[1])), dimnames = list(c(),names(plan_list)))) %\u0026gt;%\rdplyr::mutate(rundatelocal = seq.Date(date_marathon-175, date_marathon-1, by=\u0026quot;day\u0026quot;)) %\u0026gt;%\rtidyr::gather(key = label,value = value, -rundatelocal) %\u0026gt;%\rdplyr::mutate(label = as.factor(label)) %\u0026gt;%\rdplyr::mutate(value = as.integer(value))\r#\rmod \u0026lt;- depmixS4::depmix(value~label, family = poisson(link = \u0026quot;log\u0026quot;), nstates = 3, data = plan_HMM)\r#\rfm \u0026lt;- depmixS4::fit(mod, verbose = FALSE)\r#\rprobs \u0026lt;- depmixS4::posterior(fm)\r#\rplan_HMM \u0026lt;- cbind(plan_HMM,probs) %\u0026gt;%\rdplyr::select(rundatelocal,label,state) %\u0026gt;%\rtidyr::spread(label,state) %\u0026gt;%\rdplyr::select(-rundatelocal) %\u0026gt;%\rpurrr::map(~(.))\rNclust \u0026lt;- 2\rdtw_model \u0026lt;- dtwclust::tsclust(series = plan_HMM, type = \u0026quot;h\u0026quot;, k = Nclust, distance = \u0026quot;dtw_basic\u0026quot;, control = hierarchical_control(method = \u0026quot;complete\u0026quot;),\rpreproc = NULL, #args = tsclust_args(dist = list(window.size = 5L)),\rtrace = TRUE)\r#\rdtw_data \u0026lt;- ggdendro::dendro_data(dtw_model, type=\u0026quot;rectangle\u0026quot;)\r#\rlabels_order \u0026lt;- dtw_data$labels$label\r#\rdtw_result \u0026lt;- data.frame(label = names(plan_HMM), cluster = factor(stats::cutree(dtw_model, k = Nclust)))\r#\rdtw_data[[\u0026quot;labels\u0026quot;]] \u0026lt;- merge(dtw_data[[\u0026quot;labels\u0026quot;]], dtw_result, by=\u0026quot;label\u0026quot;)\rdtw_result \u0026lt;- dplyr::full_join(dtw_result,dtw_data$labels, by=c(\u0026quot;label\u0026quot;, \u0026quot;cluster\u0026quot;))%\u0026gt;%\rdplyr::arrange(x)\rcluster_box \u0026lt;- aggregate(x~cluster, ggdendro::label(dtw_data), range)\rcluster_box \u0026lt;- data.frame(cluster_box$cluster,cluster_box$x)\rcluster_threshold \u0026lt;- mean(dtw_model$height[length(dtw_model$height)-((Nclust-2):(Nclust-1))])\r#\rnumColors \u0026lt;- length(levels(dtw_result$cluster)) # How many colors you need\rgetColors \u0026lt;- scales::hue_pal() # Create a function that takes a number and returns a qualitative palette of that length (from the scales package)\rmyPalette \u0026lt;- getColors(numColors)\rnames(myPalette) \u0026lt;- levels(dtw_result$cluster) # Give every color an appropriate name\rp1 \u0026lt;- ggplot() + geom_rect(data=cluster_box, aes(xmin=X1-.3, xmax=X2+.3, ymin=0, ymax=cluster_threshold, color=cluster_box.cluster), fill=NA)+\rgeom_segment(data=ggdendro::segment(dtw_data), aes(x=x, y=y, xend=xend, yend=yend)) + coord_flip() + scale_y_continuous(\u0026quot;Distance\u0026quot;) + scale_x_continuous(\u0026quot;\u0026quot;,breaks = 1:20, labels = labels_order) + guides(color=FALSE, fill = FALSE)+\rtheme(\rpanel.grid.major = element_blank(), panel.grid.minor = element_blank(), # remove grids\rpanel.background = element_blank(), axis.text.y = element_text(colour = myPalette[dtw_result$cluster],hjust=0.5),\raxis.ticks.y=element_blank()\r)\r#\rp2 \u0026lt;- as.data.frame(matrix(unlist(plan_HMM), nrow=length(unlist(plan_HMM[1])), dimnames = list(c(),names(plan_HMM)))) %\u0026gt;%\rdplyr::mutate(rundatelocal = seq.Date(date_marathon-175, date_marathon-1, by=\u0026quot;day\u0026quot;)) %\u0026gt;%\rtidyr::gather(key = label,value = value, -rundatelocal) %\u0026gt;%\rdplyr::mutate(label = as.factor(label)) %\u0026gt;%\rdplyr::full_join(., dtw_result, by = \u0026quot;label\u0026quot;) %\u0026gt;% mutate(label = factor(label, levels = rev(as.character(labels_order)))) %\u0026gt;%\rggplot(aes(x = rundatelocal, y = value, colour = as.factor(cluster))) +\rgeom_line() +\rgeom_area(aes(fill = as.factor(cluster))) +\rcoord_cartesian(ylim = c(0,4)) +\rscale_y_continuous(name = \u0026quot;States per day [km]\u0026quot;, breaks = seq(0, 4, by = 4)) +\rscale_x_date(name = \u0026quot;Run Date\u0026quot;, date_breaks = \u0026quot;4 week\u0026quot;, date_labels = \u0026quot;%b %d\u0026quot;) +\rfacet_wrap(~label, ncol = 1, strip.position=\u0026quot;left\u0026quot;) + guides(color=FALSE, fill = FALSE) +\rtheme_bw() + theme(strip.background = element_blank(), strip.text = element_blank())\r#\rplt_list \u0026lt;- list(p2, p1)\rplt_layout \u0026lt;- rbind(c(NA, 2),\rc(1, 2),\rc(NA, 2))\r#\rgrid.arrange(grobs = plt_list, layout_matrix = plt_layout, heights = c(0.04, 1, 0.05))\rGood news this time, the clusters are almost equally distributed, bad news random plans and pattern plans are mixed together. However we can see that the HMM is creating surprisingly nice pattern which can be easily clustered with a higher number of cluster. The drawback is the low distance between each time series which can make the clustering method more complicated.\n\rDTW cluster by power spectral density\rLast but not least, the probable best approach to evaluate seasonality/frequency in training plan pattern can be the power spectrum analysis. By identifying the underlying frequencies of each time series it is possible to cluster them according their pattern. A nice new package WaveletComp can be used for this purpose. WaveletComp is analyzing the frequency structure of uni- and bivariate time series using the Morlet wavelet.\nextract_poweravg \u0026lt;- function(x){\rx \u0026lt;- as.data.frame(x)\rpower_spectrum \u0026lt;- WaveletComp::analyze.wavelet(\rmy.data = x,\rmy.series = 1,\rloess.span = 0,\rdt = 1,\rverbose = FALSE\r)\rmax_period \u0026lt;- max(power_spectrum$Period)\rdat \u0026lt;- spline(power_spectrum$Power.avg, n = max_period)$y # WARNING:power band starts at 2 not 1\rreturn(dat)\r}\rplan_poweravge \u0026lt;- plan_list %\u0026gt;%\rpurrr::map(~extract_poweravg(.))\rNclust \u0026lt;- 2\rdtw_model \u0026lt;- dtwclust::tsclust(series = plan_poweravge, type = \u0026quot;h\u0026quot;, k = Nclust, distance = \u0026quot;dtw_basic\u0026quot;, control = hierarchical_control(method = \u0026quot;complete\u0026quot;),\rpreproc = NULL, #args = tsclust_args(dist = list(window.size = 5L)),\rtrace = TRUE)\r#\rdtw_data \u0026lt;- ggdendro::dendro_data(dtw_model, type=\u0026quot;rectangle\u0026quot;)\r#\rlabels_order \u0026lt;- dtw_data$labels$label\r#\rdtw_result \u0026lt;- data.frame(label = names(plan_poweravge), cluster = factor(stats::cutree(dtw_model, k = Nclust)))\r#\rdtw_data[[\u0026quot;labels\u0026quot;]] \u0026lt;- merge(dtw_data[[\u0026quot;labels\u0026quot;]], dtw_result, by=\u0026quot;label\u0026quot;)\rdtw_result \u0026lt;- dplyr::full_join(dtw_result,dtw_data$labels, by=c(\u0026quot;label\u0026quot;, \u0026quot;cluster\u0026quot;))%\u0026gt;%\rdplyr::arrange(x)\rcluster_box \u0026lt;- aggregate(x~cluster, ggdendro::label(dtw_data), range)\rcluster_box \u0026lt;- data.frame(cluster_box$cluster,cluster_box$x)\rcluster_threshold \u0026lt;- mean(dtw_model$height[length(dtw_model$height)-((Nclust-2):(Nclust-1))])\r#\rnumColors \u0026lt;- length(levels(dtw_result$cluster)) # How many colors you need\rgetColors \u0026lt;- scales::hue_pal() # Create a function that takes a number and returns a qualitative palette of that length (from the scales package)\rmyPalette \u0026lt;- getColors(numColors)\rnames(myPalette) \u0026lt;- levels(dtw_result$cluster) # Give every color an appropriate name\rp1 \u0026lt;- ggplot() + geom_rect(data=cluster_box, aes(xmin=X1-.3, xmax=X2+.3, ymin=0, ymax=cluster_threshold, color=cluster_box.cluster), fill=NA)+\rgeom_segment(data=ggdendro::segment(dtw_data), aes(x=x, y=y, xend=xend, yend=yend)) + coord_flip() + scale_y_continuous(\u0026quot;Distance\u0026quot;) + scale_x_continuous(\u0026quot;\u0026quot;,breaks = 1:20, labels = labels_order) + guides(color=FALSE, fill = FALSE)+\rtheme(\rpanel.grid.major = element_blank(), panel.grid.minor = element_blank(), # remove grids\rpanel.background = element_blank(), axis.text.y = element_text(colour = myPalette[dtw_result$cluster],hjust=0.5),\raxis.ticks.y=element_blank()\r)\r#\rp2 \u0026lt;- as.data.frame(matrix(unlist(plan_poweravge), nrow=length(unlist(plan_poweravge[1])), dimnames = list(c(),names(plan_poweravge)))) %\u0026gt;%\rdplyr::mutate(rundatelocal = 1:n()) %\u0026gt;%\rtidyr::gather(key = label,value = value, -rundatelocal) %\u0026gt;%\rdplyr::mutate(label = as.factor(label)) %\u0026gt;%\rdplyr::full_join(., dtw_result, by = \u0026quot;label\u0026quot;) %\u0026gt;% mutate(label = factor(label, levels = rev(as.character(labels_order)))) %\u0026gt;%\rggplot(aes(x = rundatelocal, y = value, colour = as.factor(cluster))) +\rgeom_line() +\rgeom_area(aes(fill = as.factor(cluster))) +\rcoord_cartesian(ylim = c(0,1)) +\rscale_y_continuous(name = \u0026quot;Average power density\u0026quot;, breaks = seq(0, 1, by = 1)) +\rscale_x_continuous(name = \u0026quot;Period (days)\u0026quot;) +\rfacet_wrap(~label, ncol = 1, strip.position=\u0026quot;left\u0026quot;) + guides(color=FALSE, fill = FALSE) +\rtheme_bw() + theme(strip.background = element_blank(), strip.text = element_blank())\r#\rplt_list \u0026lt;- list(p2, p1)\rplt_layout \u0026lt;- rbind(c(NA, 2),\rc(1, 2),\rc(NA, 2))\r#\rgrid.arrange(grobs = plt_list, layout_matrix = plt_layout, heights = c(0.04, 1, 0.05))\rThis frequency decomposition looks amazing but be careful because the power frequency are average and as stated in “WaveletComp 1.1:A guided tour through the R package”, “The average power plot cannot distinguish between consecutive periods and overlapping periods”. This is annoying but average power is definitely a first step toward a nice classification of training plan patterns.\n\r\r","date":1538006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538006400,"objectID":"0a1826fc025f845a763ae67a24d65bc6","permalink":"/post/time-series-clustering-with-dynamic-time-warping-part-2/","publishdate":"2018-09-27T00:00:00Z","relpermalink":"/post/time-series-clustering-with-dynamic-time-warping-part-2/","section":"post","summary":"Like every good movies, my previous blog post “Time series clustering with Dynamic Time Warping” deserves a sequel. In this Part 2, I will have a look at the athletes’ training plan for a marathon.","tags":[],"title":"Time series clustering with Dynamic Time Warping (Part 2)","type":"post"},{"authors":null,"categories":[],"content":"\rRecently I was asked to make a short presentation about how to introduce the idea of statistical test and statistical significance to students in marketing. The timing was very good because I just had red a blog post written by Stephen B. Heard called Why do we make statistics so hard for our students? which is exactly the way to follow. So I taken the main idea of this post and made a short presentation with R that you can find here:\n\rI’ve also taken a simple meme which looks very trendy on the net. A great sentence by W. Edwards Deming “Without data you’re just another person with an opinion”. It’s simple but it is the reason why Statistics are very important not only in academia but also in companies.\nPS: Special thanks to Tim Mastny for his blog post about how to host R presentation in a blogdown\n","date":1536796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536796800,"objectID":"ecbe24f0906a5b6683f1dee3db1b76fb","permalink":"/post/a-simple-introduction-to-statistical-test-and-statistical-significance/","publishdate":"2018-09-13T00:00:00Z","relpermalink":"/post/a-simple-introduction-to-statistical-test-and-statistical-significance/","section":"post","summary":"Recently I was asked to make a short presentation about how to introduce the idea of statistical test and statistical significance to students in marketing. The timing was very good because I just had red a blog post written by Stephen B.","tags":[],"title":"A simple introduction to statistical test and statistical significance","type":"post"},{"authors":null,"categories":[],"content":"\rMany solutions for clustering time series are available with R and as usual the web is full of nice tutorials like Thomas Girke’s blog post, Rafael Irizarry and Michael Love’s book, Andrew B. Collier’s blog post, Peter Laurinec’s blog post, Dylan Glotzer’s lecture or Ana Rita Marques’s module.\nDynamic Time Warping (DTW) is one of these solutions. The main advantage of DTW is the possibility to group time series according their patterns or shapes even if these patterns are not synchronized (lag).\nAs far as I know the two main packages which allow time series clustering with DTW are TSclust by Pablo Montero Manso and José Antonio Vilar and dtwclust by Alexis Sarda-Espinosa. These packages are very simple but powerful tools to analyse time series. However when it comes to analyse real data, I found difficult to understand how the clustering is working. To make this process clearer I’m going to simulate two groups of time series and to check if whether or not the DTW clustering can differentiate them.\nList of packages needed\rlibrary(dplyr) # data wrangling\rlibrary(ggplot2) # grammar of graphics\rlibrary(gridExtra) # merge plots\rlibrary(ggdendro) # dendrograms\rlibrary(gplots) # heatmap\rlibrary(tseries) # bootstrap\rlibrary(TSclust) # cluster time series\rlibrary(dtwclust) # cluster time series with dynamic time warping\r\rData simulation\rLet’s imagine two people running a marathon, one had a classic run with a pace increasing with the time and the other had a very bad experience (e.g. “hitting the wall”) with a jump in the pace which indicates a significant slow down in the second part of the run. The best is to have real data to analyse but it can be very useful to simulate these pattern in order to assess the clustering efficiency.\nA simple way to simulate these time series is to use the sine function and to add a random noise in order to make it more credible.\n# classic run\rnoise \u0026lt;- runif(420) # random noise\rx \u0026lt;- seq(1,420) # 42km with a measure every 100m\rpace_min \u0026lt;- 5 # min/km (corresponds to fast run)\rts_sim_classic_run \u0026lt;- (sin(x/10)+x/100+noise+pace_min) %\u0026gt;%\ras.ts(.)\rts.plot(ts_sim_classic_run, xlab = \u0026quot;Distance [x100m]\u0026quot;, ylab = \u0026quot;Differential pace [min/km]\u0026quot;, main = \u0026quot;Example of classic run\u0026quot;, ylim=c(0,25))\r# wall run\rnoise \u0026lt;- runif(210) # random noise\rx \u0026lt;- seq(1,210) # 21km with a measure every 100m pace_min \u0026lt;- 5 # min/km (corresponds to fast run)\rpace_wall \u0026lt;- 20 # min/km (corresponds to very slow run) ts_sim_part1 \u0026lt;- sin(x/5)+x/50+noise+pace_min\rts_sim_part2 \u0026lt;- sin(x/5)+noise+pace_wall\rts_sim_wall_run \u0026lt;- c(ts_sim_part1,ts_sim_part2) %\u0026gt;%\ras.ts(.)\rts.plot(ts_sim_wall_run, xlab = \u0026quot;Distance [x100m]\u0026quot;, ylab = \u0026quot;Differential pace [min/km]\u0026quot;, main = \u0026quot;Example of wall run\u0026quot;, ylim=c(0,25))\rA much nicer way would be to use ARIMA with an auto regressive model (AR).\npace_min \u0026lt;- 5 # min/km (corresponds to fast run)\rpace_wall \u0026lt;- 20 # min/km (corresponds to very slow run) # classic run\rts_sim_classic_run \u0026lt;- abs(arima.sim(n = 420, mean = 0.001, model = list(order = c(1,0,0), ar = 0.9))) + pace_min\rts.plot(ts_sim_classic_run, xlab = \u0026quot;Distance [x100m]\u0026quot;, ylab = \u0026quot;Differential pace [min/km]\u0026quot;, main = \u0026quot;Example of classic run\u0026quot;, ylim=c(0,25))\r# wall run\rts_sim_part1 \u0026lt;- abs(arima.sim(n = 210, model = list(order = c(1,0,0), ar = 0.9))) + pace_min\rts_sim_part2 \u0026lt;- ts(arima.sim(n = 210, model = list(order = c(1,0,0), ar = 0.9)) + pace_wall, start = 211,end =420)\rts_sim_wall_run \u0026lt;- ts.union(ts_sim_part1,ts_sim_part2)\rts_sim_wall_run\u0026lt;- pmin(ts_sim_wall_run[,1], ts_sim_wall_run[,2], na.rm = TRUE)\rts.plot(ts_sim_wall_run, xlab = \u0026quot;Distance [x100m]\u0026quot;, ylab = \u0026quot;Differential pace [min/km]\u0026quot;, main = \u0026quot;Example of wall run\u0026quot;, ylim=c(0,25))\r\rBootstrap\rNow we have two different runs, let’s bootstrap them (i.e. replicate with small differences) in order to have two groups of 5 individuals for each run type.\nts_sim_boot_classic \u0026lt;- ts_sim_classic_run %\u0026gt;%\rtseries::tsbootstrap(., nb=5, b=200, type = \u0026quot;block\u0026quot;) %\u0026gt;%\ras.data.frame(.) %\u0026gt;%\rdplyr::rename_all(funs(c(paste0(\u0026quot;classic_\u0026quot;,.))))\rts_sim_boot_wall \u0026lt;- ts_sim_wall_run %\u0026gt;%\rtseries::tsbootstrap(., nb=5, b=350, type = \u0026quot;block\u0026quot;) %\u0026gt;%\ras.data.frame(.) %\u0026gt;%\rdplyr::rename_all(funs(c(paste0(\u0026quot;wall_\u0026quot;,.))))\rts_sim_df \u0026lt;- cbind(ts_sim_boot_classic,ts_sim_boot_wall)\r\rHeatmap cluster\rEven if I’m a big fan of ggplot2 possibilities, some packages offer efficient ways to compute and plot data. For heatmaps I’m using the gplots package which displays time series with dendrograms is a single function. An overlook of all the heatmap possibilities can be found here\ndtw_dist \u0026lt;- function(x){dist(x, method=\u0026quot;DTW\u0026quot;)}\rts_sim_df %\u0026gt;%\ras.matrix() %\u0026gt;%\rgplots::heatmap.2 (\r# dendrogram control\rdistfun = dtw_dist,\rhclustfun = hclust,\rdendrogram = \u0026quot;column\u0026quot;,\rRowv = FALSE,\rlabRow = FALSE\r)\rWe can already see an accurate clustering between classic and wall runs but we are interested in DTW analysis so let’s implement TSclust and dtwclust packages.\n\rDTW cluster\rBoth TSclust and dtwclust are following the same steps:\nCalculating the difference between each time series using the DTW method (but many other distances can be calculated, see for example Montero \u0026amp; Vilar, 2014).\rCalculating hierarchical cluster analysis over these dissimilarities.\rPlotting a dendrogram to visually assess the cluster accuracy. The solution to plot the time series with the dendrogram was taken from Ian Hansel’s blog.\r\rUsing TSclust\r# cluster analysis\rdist_ts \u0026lt;- TSclust::diss(SERIES = t(ts_sim_df), METHOD = \u0026quot;DTWARP\u0026quot;) # note the dataframe must be transposed\rhc \u0026lt;- stats::hclust(dist_ts, method=\u0026quot;complete\u0026quot;) # meathod can be also \u0026quot;average\u0026quot; or diana (for DIvisive ANAlysis Clustering)\r# k for cluster which is 2 in our case (classic vs. wall)\rhclus \u0026lt;- stats::cutree(hc, k = 2) %\u0026gt;% # hclus \u0026lt;- cluster::pam(dist_ts, k = 2)$clustering has a similar result\ras.data.frame(.) %\u0026gt;%\rdplyr::rename(.,cluster_group = .) %\u0026gt;%\rtibble::rownames_to_column(\u0026quot;type_col\u0026quot;)\rhcdata \u0026lt;- ggdendro::dendro_data(hc)\rnames_order \u0026lt;- hcdata$labels$label\r# Use the folloing to remove labels from dendogram so not doubling up - but good for checking hcdata$labels$label \u0026lt;- \u0026quot;\u0026quot;\rp1 \u0026lt;- hcdata %\u0026gt;%\rggdendro::ggdendrogram(., rotate=TRUE, leaf_labels=FALSE)\rp2 \u0026lt;- ts_sim_df %\u0026gt;%\rdplyr::mutate(index = 1:420) %\u0026gt;%\rtidyr::gather(key = type_col,value = value, -index) %\u0026gt;%\rdplyr::full_join(., hclus, by = \u0026quot;type_col\u0026quot;) %\u0026gt;% mutate(type_col = factor(type_col, levels = rev(as.character(names_order)))) %\u0026gt;% ggplot(aes(x = index, y = value, colour = cluster_group)) +\rgeom_line() +\rfacet_wrap(~type_col, ncol = 1, strip.position=\u0026quot;left\u0026quot;) + guides(color=FALSE) +\rtheme_bw() + theme(strip.background = element_blank(), strip.text = element_blank())\rgp1\u0026lt;-ggplotGrob(p1)\rgp2\u0026lt;-ggplotGrob(p2) grid.arrange(gp2, gp1, ncol=2, widths=c(4,2))\rAs expected, The results of TSclustshow two different groups, one with the classic runs and one with wall runs. However we can see that wall runs are not sorted perfectly according their shape. Let’s have a look at dtwclust to see if the results are similar.\n\rUsing dtwclust\rThe main asset of dtwclust is the possibility to customize the DTW clustering. For more details about all the possibilities, I suggest to have a look at the dtwclust package vignette.\ncluster_dtw_h2 \u0026lt;- dtwclust::tsclust(t(ts_sim_df), type = \u0026quot;h\u0026quot;, k = 2, distance = \u0026quot;dtw\u0026quot;, control = hierarchical_control(method = \u0026quot;complete\u0026quot;),\rpreproc = NULL, args = tsclust_args(dist = list(window.size = 5L)))\rhclus \u0026lt;- stats::cutree(cluster_dtw_h2, k = 2) %\u0026gt;% # hclus \u0026lt;- cluster::pam(dist_ts, k = 2)$clustering has a similar result\ras.data.frame(.) %\u0026gt;%\rdplyr::rename(.,cluster_group = .) %\u0026gt;%\rtibble::rownames_to_column(\u0026quot;type_col\u0026quot;)\rhcdata \u0026lt;- ggdendro::dendro_data(cluster_dtw_h2)\rnames_order \u0026lt;- hcdata$labels$label\r# Use the folloing to remove labels from dendogram so not doubling up - but good for checking hcdata$labels$label \u0026lt;- \u0026quot;\u0026quot;\rp1 \u0026lt;- hcdata %\u0026gt;%\rggdendro::ggdendrogram(., rotate=TRUE, leaf_labels=FALSE)\rp2 \u0026lt;- ts_sim_df %\u0026gt;%\rdplyr::mutate(index = 1:420) %\u0026gt;%\rtidyr::gather(key = type_col,value = value, -index) %\u0026gt;%\rdplyr::full_join(., hclus, by = \u0026quot;type_col\u0026quot;) %\u0026gt;% mutate(type_col = factor(type_col, levels = rev(as.character(names_order)))) %\u0026gt;% ggplot(aes(x = index, y = value, colour = cluster_group)) +\rgeom_line() +\rfacet_wrap(~type_col, ncol = 1, strip.position=\u0026quot;left\u0026quot;) + guides(color=FALSE) +\rtheme_bw() + theme(strip.background = element_blank(), strip.text = element_blank())\rgp1\u0026lt;-ggplotGrob(p1)\rgp2\u0026lt;-ggplotGrob(p2) grid.arrange(gp2, gp1, ncol=2, widths=c(4,2))\rNow with the cluster are well distributed between classic and wall runs but also inside the clusters where similar shapes appears to be grouped together.\nIt is possible to modify some argument in order to perform this hierarchical DTW clustering based on z-scores with centroid based on the built-in “shape_extraction” function.\ncluster_dtw_h2 \u0026lt;- dtwclust::tsclust(t(ts_sim_df), type = \u0026quot;h\u0026quot;, k = 2L,\rpreproc = zscore,\rdistance = \u0026quot;dtw\u0026quot;, centroid = shape_extraction,\rcontrol = hierarchical_control(method = \u0026quot;complete\u0026quot;))\rhclus \u0026lt;- stats::cutree(cluster_dtw_h2, k = 2) %\u0026gt;% # hclus \u0026lt;- cluster::pam(dist_ts, k = 2)$clustering has a similar result\ras.data.frame(.) %\u0026gt;%\rdplyr::rename(.,cluster_group = .) %\u0026gt;%\rtibble::rownames_to_column(\u0026quot;type_col\u0026quot;)\rhcdata \u0026lt;- ggdendro::dendro_data(cluster_dtw_h2)\rnames_order \u0026lt;- hcdata$labels$label\r# Use the folloing to remove labels from dendogram so not doubling up - but good for checking hcdata$labels$label \u0026lt;- \u0026quot;\u0026quot;\rp1 \u0026lt;- hcdata %\u0026gt;%\rggdendro::ggdendrogram(., rotate=TRUE, leaf_labels=FALSE)\rp2 \u0026lt;- ts_sim_df %\u0026gt;%\rdplyr::mutate(index = 1:420) %\u0026gt;%\rtidyr::gather(key = type_col,value = value, -index) %\u0026gt;%\rdplyr::full_join(., hclus, by = \u0026quot;type_col\u0026quot;) %\u0026gt;% mutate(type_col = factor(type_col, levels = rev(as.character(names_order)))) %\u0026gt;% ggplot(aes(x = index, y = value, colour = cluster_group)) +\rgeom_line() +\rfacet_wrap(~type_col, ncol = 1, strip.position=\u0026quot;left\u0026quot;) + guides(color=FALSE) +\rtheme_bw() + theme(strip.background = element_blank(), strip.text = element_blank())\rgp1\u0026lt;-ggplotGrob(p1)\rgp2\u0026lt;-ggplotGrob(p2) grid.arrange(gp2, gp1, ncol=2, widths=c(4,2))\rBased on dtwclust package vignette, it is possible to register a new DTW function adapted to normalized and asymmetric DTW.\n# Normalized DTW\rndtw \u0026lt;- function(x, y, ...) {\rdtw(x, y, ...,\rstep.pattern = asymmetric,\rdistance.only = TRUE)$normalizedDistance\r}\r# Register the distance with proxy\rproxy::pr_DB$set_entry(FUN = ndtw, names = c(\u0026quot;nDTW\u0026quot;),\rloop = TRUE, type = \u0026quot;metric\u0026quot;, distance = TRUE,\rdescription = \u0026quot;Normalized, asymmetric DTW\u0026quot;)\r# Partitional clustering\rcluster_dtw_h2 \u0026lt;- dtwclust::tsclust(t(ts_sim_df), k = 2L,distance = \u0026quot;nDTW\u0026quot;)\rplot(cluster_dtw_h2)\rEven if it looks great with sine simulated data, it is not very accurate with ARIMA models. Moreover I haven’t been able to extract the dendrogram from this last “cluster_dtw_h2” object because of the partitional clustering process but one can be interested in the distance matrix provided in “cluster_dtw_h2” object.\nAfter this short analysis with Dynamic Time Warping, the next steps will be to increase the difference between the time series to check the clustering accuracy and obviously to test it with real data.\n\r\r","date":1536192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536192000,"objectID":"a6988f08c9d9c163fc3fcbf64887e8fa","permalink":"/post/time-series-clustering-with-dynamic-time-warp/","publishdate":"2018-09-06T00:00:00Z","relpermalink":"/post/time-series-clustering-with-dynamic-time-warp/","section":"post","summary":"Many solutions for clustering time series are available with R and as usual the web is full of nice tutorials like Thomas Girke’s blog post, Rafael Irizarry and Michael Love’s book, Andrew B.","tags":[],"title":"Time series clustering with Dynamic Time Warping","type":"post"},{"authors":null,"categories":[],"content":"\rFinally! My first post to my website is about to be live on the web! I’m always amazed by the possibilities offered by R and Rstudio and my first lines will thank this huge community which make the awesomeness real. However it’s not always easy to use and to apply these magnificent tools. Publishing a blogdown is a good example of a process that looks easy on the first sight but which can be tricky. Here are some insights from my experience in publishing a research blog from blogdown.\nFirst steps with blogdown\rThe blogdown package is very well documented and if you are intended to publish your own websites the best approach is to have a look at the bookdown made by Yihui Xie, Amber Thomas and Alison Presmanes Hill blogdown: Creating Websites with R Markdown.\nSeveral tutorials are available to help new users in this case:\n\rA blog post writen by Alison Presmanes Hill, Up and running with blogdown, is a very good first step to blogdown. The post is well documented and will help to solve most of the major problems to setup a website.\rThe talk given by Yihui Xie at the rstudio::conf 2018, Create and maintain websites with R Markdown and blogdown, is short and very well presented.\rThe youtube tutorial by John Muschelli, Making a Website with Blogdown, presents all the steps from building a website to publishing it on netlify.com.\r\r\rProblem solving\rEven if the authors of blogdown made it very easy to publish a blog from Rstudio, some problem can be encountered while following the basic steps. Here is my experience in troubleshooting some of the problems that can happen.\nPrerequisite\rIn order to avoid the majority of the problems that may happen, it is essential to have the latest versions of R, Rstudio, blogdown package (don’t hesitate to use its GitHub version devtools::install_github(\u0026quot;rstudio/blogdown\u0026quot;)) and Hugo (blogdown::update_hugo()). Like mine, if the process of publishing a blogdown takes several weeks, upgrading to the latest version for every try can slove a lot of problems.\n\rCreating a GitHub connection\rIf one wants to make blogdown live on the web, a GitHub integration is the way to go according to me. Two routes are possible:\nCreating a blogdown project from Rstudio and then uploading the project on GitHub. Rstudio GUI is one of its main asset and creating a blogdown project from Rstudio is very easy but the connection with GitHub has to be done afterward which can lead to several problems when committing the changes.\rCreating an empty repo on GitHub, then creating a new project with version control on Rstudio and use the function blogdown::build_site() in this new project. This solution doesn’t look as straight forward but once it is done, the connection with GitHub is very stable.\r\r\rInstalling a new theme\rWhen it comes to build the site, it is possible to specify the theme you want to use with blogdown::new_site(theme = \u0026quot;gcushen/hugo-academic\u0026quot;). Even if the default theme is nice, researchers and students can find the theme “academic” more suitable. But be sure to have installed a version of Hugo that corresponds with the theme version of Hugo.\nAfter this initial command, the template website should be displayed in Rstudio viewer. If you have to close the project it is possible to relaunch this view using the Serve Site shortcut in Rstudio Addins or by using blogdown::serve_site(). Next step is the uploading of this template website in GitHub.\n\rMaking GitHub commit and push\rA big advantage of GitHub integration is that when your website is live on the web, any new post or new content is automatically uploaded with simple commit and push from Rstudio project. Rstudio provides a nice interface to do these commit and push to GitHub without using any command lines which is great for new coders and/or windows users. However with the initial commit and push of a blogdown I didn’t manage to use Rstudio GUI resulting of endless lags and multiple reboots. The solution is to use github commit and push from command lines.\nif you are new to command lines don’t be afraid, it is very easy:\nIf your project is linked with GitHub, that means Git is also installed and Git comes with a CMD prompt that can be used for manual commit and push to GitHub.\rChange the directory\r\rfrom C:\\Users\\MyName\u0026gt;\rto C:\\Users\\MyName\\MyFolderName\\MyBlogdownProjectName\u0026gt;\rusing cd .\\MyFolderName\\MyBlogdownProjectName.\r\rAdd all changes with the command git add -A.\rCommit these changes with the command git commit -m \u0026quot;initial commit text\u0026quot;.\rPush these changes to GitHub with the command git push.\r\rNote: these commands are the most basic ones, it can me more complicated to add only specific files or to push from another branch but in these cases you can easily find the commands on the web.\n\rPublishing the template to Netlify\rOnce this initial commit is done, it is possible to make the template live on Netlify. I think it is a good idea to not modify the template yet in order to first check how Netlify is handling the template website.\nOn Netlify, with GitHub login and password, it is easy to find you repo and to deploy the website. However Netlify will analyse the project and this can result in more errors and problems. The biggest problem that I had was solved very easily with the genius Mara Averick and her blog post Updating your version of Hugo for blogdown on Netlify. I should create a blog dedicated to how amazing Mara is and as usual she solved a massive problem. Indeed, Hugo’s theme have specific minimum Hugo version to use but even Rstudio is using the latest version to build the website, Netlify needs to know which version to use. The correct version must be set as a New variable with the key to HUGO_VERSION. Then the magic happens and the website is deployed to the world. the first URL address of the website is quite complicated but Netlify allows to change it for free.\n\rChanging the content of the template\rLast but not least, the website you have just published needs to be filled with your own content. The first step will be to replace all the lines in the sub-folder home with your own data and to delete all the posts, publications, citations, images, etc. inside the template. However it is still possible to broke the website by deleting a useful link. In this case I have two advice:\nKeep a version of the template in case you need to get back one of the files and then understanding why deleting these information is braking the website.\rDo not delete the files called _index.md, they are very useful to create the public site of the website.\r\r\r\rWhat’s next?\rThe websites created with the blogdown package from the R and stats community are united in a project called rbind. I hope to be one day expert enough to be part of this community and hopefully with my tricks and tips, you will as well ;)\n\r","date":1536019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536019200,"objectID":"4325d7054682143c838324406f820b55","permalink":"/post/publishing-blogdown-some-insights-from-my-own-experience/","publishdate":"2018-09-04T00:00:00Z","relpermalink":"/post/publishing-blogdown-some-insights-from-my-own-experience/","section":"post","summary":"Finally! My first post to my website is about to be live on the web! I’m always amazed by the possibilities offered by R and Rstudio and my first lines will thank this huge community which make the awesomeness real.","tags":[],"title":"Publishing blogdown, some insights from my own experience","type":"post"},{"authors":["Damien Dupré","Nicole Andelic","Anna Zajac","Gawain Morrison","Gary McKeown"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"b310d181aea2f2e2023bacd44f0cda8a","permalink":"/publication/2018-socialmedia-psyarxiv/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/2018-socialmedia-psyarxiv/","section":"publication","summary":"Sharing personal information is an important way of communicating on social media. Among the information possibly shared, new sensors and tools allow people to share emotion information via facial emotion recognition. This paper questions whether people are prepared to share personal information such as their own emotion on social media. In the current study we examined how factors such as felt emotion, motivation for sharing on social media as well as personality affected participants’ willingness to share self-reported emotion or facial expression online. By carrying out a GLMM analysis, this study found that participants’ willingness to share self-reported emotion and facial expressions was influenced by their personality traits and the motivation for sharing their emotion information that they were given. From our results we can conclude that the estimated level of privacy for certain emotional information, such as facial expression, is influenced by the motivation for sharing the information online.","tags":null,"title":"The effect of personality and social context on willingness to share emotion information on social media","type":"publication"},{"authors":["Damien Dupré","Michel Dubois","Anna Tcherkassof"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"6066820de43f1977f9eb1817c7d34a8f","permalink":"/publication/2017-acceptance-pto/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/publication/2017-acceptance-pto/","section":"publication","summary":"To anticipate a product commercial success, companies aim to ensure that product's characteristics are involving users and triggering a positive first user experience as well as all along product's life span. Among these characteristics, triggering users’ emotions is particularly relevant. This article aims to present the different approaches integrating the emotion generated by products. In order to illustrate the emotional potential of certain products a selection of 14 photographs of products was evaluated by 87 participants. Results reveal a significant correlation between cognitive, motivational and subjective components of emotions. A reflexion about the integration of cognitive appraisal assessment and motivational action readiness evaluation into a wilder emotional model is provided. Thus, emotions triggered by a product appear to be a stake for designers and an important variable to introduce in acceptability or user experience models.","tags":null,"title":"Role of emotions in product acceptance: Evaluation of the cognitive, motivational and subjective component","type":"publication"},{"authors":null,"categories":null,"content":"SYMPOSIUM 2: Affect Recognition in Humans versus Machines: Current Issues and Future Challenges Convener: Eva Krumhuber, University College London, UK\nABSTRACT: Automatic facial expression recognition systems can provide important information about our emotions and how they change over time. While the use of automatic systems has seen a steady increase over the last years, their classification results have not yet been systematically compared. The aim of this research was to test commercial software packages from Affectiva, Kairos and Microsoft companies in terms of their recognition accuracy. For this, we focused on spontaneous and dynamic facial expressions as provided by the DynEmo database –Disgust, Fear, Joy, and Surprise. In order to compare the classification results, we calculated the systems’ ratio of True positives (only the target label is recognised), False positives (the target label as well as a non-target label is recognised), True negatives (no label is recognised) and False negatives (target label is not recognised whereas a non-target label is). The results of the comparison between the systems showed comparable detection rates in term of True positives and False positives. However, their detection rates of False negatives and True negatives significantly differed between the different recognition systems. Specifically, systems were not equal in their tendency to detect non-target labels erroneously as well as in their tendency to not detect any emotion label. When examining emotion recognition accuracy for each video/emotion, videos with higher recognition accuracy were those that depicted as joyful facial expression. Other facial expressions resulted in a proportion of target emotion detection statistically equal or lower than the detection of non-target emotion. These results suggest that systems are not equivalent in their ability to detect specific spontaneous emotions. Therefore, users of such systems have to be aware of the strengths as well as of the potential limits of the data provided by automatic emotion recognition systems.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"829168eaa641e73affffe98045950239","permalink":"/talk/2018-cere/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2018-cere/","section":"talk","summary":"SYMPOSIUM 2: Affect Recognition in Humans versus Machines: Current Issues and Future Challenges Convener: Eva Krumhuber, University College London, UK\nABSTRACT: Automatic facial expression recognition systems can provide important information about our emotions and how they change over time.","tags":null,"title":"A Comparison of Three Commercial Systems for Automatic Recognition of Spontaneous Facial Expressions","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT: Voice is one of the main communicative sources of evidence in interpreting the expression of emotion. Affective computing aims to create systems and algorithms that automatically analyse people\u0026rsquo;s emotional state. Consequently, several companies such as Affectiva, Beyond Verbal and Audeering have developed automatic systems to analyse the vocal expression of emotions. However, little is known about the accuracy of such systems. To evaluate the accuracy of automatic emotion recognition from voice, we processed vocal expressions from the GEMEP database with \u0026ldquo;SensAI Emotion\u0026rdquo; developed by Audeering. The GEMEP database contains audio-video recordings from 10 actors performing 17 different emotional scenarios (Bänziger \u0026amp; Scherer, 2010). SensAI Emotion analyses emotions from speech and renders a value for 23 affective states and for valence and arousal dimensions (Eyben, Scherer, \u0026amp; Schuller, 2018). In terms of category recognition, the accuracy of SensAI labeling GEMEP vocal expressions of emotion is 6.67%. However, this low result is partly due to the high number of different affective state labels recognized. To bypass this label matching bias, we compared the recognition accuracy for valence and arousal dimensions. The results show an accuracy of 0.56 (CI95%[0.46,0.65]) for valence and 0.73 (CI95%[0.64,0.81]) for arousal recognition. Vocal automatic emotion recognition is a growing research area in affective computing. The categorical recognition of emotion remains a challenge due to the diversity of affective states. However, the accuracy of a system like SensAI Emotion provides promising results in the recognition of valence and arousal.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"0bf85825180a75dcb9d3cd7d3ddb6da9","permalink":"/talk/2019-isre/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2019-isre/","section":"talk","summary":"ABSTRACT: Voice is one of the main communicative sources of evidence in interpreting the expression of emotion. Affective computing aims to create systems and algorithms that automatically analyse people\u0026rsquo;s emotional state.","tags":null,"title":"Accuracy of Automatic Emotion Recognition from Voice","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT:\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"ec18e2d1833edc04351fa65b592e3653","permalink":"/talk/2018-percom/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2018-percom/","section":"talk","summary":"ABSTRACT:","tags":null,"title":"Accuracy of three commercial automatic emotion recognition systems across different individuals and their facial expressions","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT:\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"afb9ac9d32b42efd8c5ce6de8b90bbe8","permalink":"/talk/2014-cere/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2014-cere/","section":"talk","summary":"ABSTRACT:","tags":null,"title":"Are approach-avoidance relevant cues of the Emotional User eXperience? Case studies with innovative products","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT: With the development of wearable sensors, it is now possible to assess the dynamic progression of physiological rhythms such as heart rate, breathing rate or galvanic skin response in ways and places and during certain activities that were previously not possible due to cost and reliability. This paper investigates the physiological changes when participating in a zipline activity. Despite the advances in sensor technology, the statistical analysis of such physiological signals remains a challenge for data analysts. This paper presents a workflow encompassing the whole process with the goal of obtaining a range of best fitting models to analyse the patterns given by these measurements “in the wild”.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"162546dfedc5ea418731cbad6a829a79","permalink":"/talk/2017-aisb/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2017-aisb/","section":"talk","summary":"ABSTRACT: With the development of wearable sensors, it is now possible to assess the dynamic progression of physiological rhythms such as heart rate, breathing rate or galvanic skin response in ways and places and during certain activities that were previously not possible due to cost and reliability.","tags":null,"title":"Dynamic Analysis of Automatic Emotion Recognition Using Generalized Additive Mixed Models","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT: The analysis of facial expressions is currently a favoured method of inferring experienced emotion, and consequently significant efforts are currently being made to develop improved facial expression recognition techniques. Among these new techniques, those which allow the automatic recognition of facial expression appear to be most promising. This paper presents a new method of facial expression analysis with a focus on the continuous evolution of emotions using Generalized Additive Mixed Models (GAMM) and Significant Zero Crossing of the Derivatives (SiZer). The time-series analysis of the emotions experienced by participants watching a series of three different online videos suggests that analysis of facial expressions at the overall level may lead to misinterpretation of the emotional experience whereas non-linear analysis allows the significant expressive sequences to be identified.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"bb71d58a6ea9cc17ac083952cd366d61","permalink":"/talk/2018-bhci/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2018-bhci/","section":"talk","summary":"ABSTRACT: The analysis of facial expressions is currently a favoured method of inferring experienced emotion, and consequently significant efforts are currently being made to develop improved facial expression recognition techniques. Among these new techniques, those which allow the automatic recognition of facial expression appear to be most promising.","tags":null,"title":"Dynamic Analysis of Automatic Facial Expressions Recognition ‘in the Wild’ Using Generalized Additive Mixed Models and Significant Zero Crossing of the Derivatives","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT: With the development of wearable sensors, it is now possible to assess the dynamic progression of physiological rhythms such as heart rate, breathing rate or galvanic skin response in ways and places that were previously impractical. This paper presents a new application that synchronizes the emotional patterns from these time-series in order to model athletes’ emotion during physical activity. This data analysis computes a best-fitting model for analyzing the patterns given by these measurements “in the wild”. The recording setup used to measure and synchronize multiple biometric physiological sensors can be called a BAN (Body Area Network) of personal measurements. By monitoring physical activity, it is now possible to calculate optimal patterns for managing athletes’ emotion. The data provided here are not restricted by a lab environment but close to the “ground truth” of ecologically valid physiological changes. The data allow the provision of accurate feedback to athletes about their emotion (e.g. in cases such as an unexpected increase or an expected decrease of physiological activity).\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"ade35b7546c48c5d89a3a9910b1b1094","permalink":"/talk/2017-ahfe/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2017-ahfe/","section":"talk","summary":"ABSTRACT: With the development of wearable sensors, it is now possible to assess the dynamic progression of physiological rhythms such as heart rate, breathing rate or galvanic skin response in ways and places that were previously impractical.","tags":null,"title":"Dynamic Model of Athletes’ Emotions Based on Wearable Devices","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT: User eXperience studies with products, systems or services have significantly increased in companies in order to anticipate their commercial success. Among the user experience dimensions, emotions are predominant. However User eXperience studies used several concepts to refer to emotions and current measures still have some flaws. Consequently, this doctoral project aims firstly to provide a multi-componential approach of emotions based on a psychological view, and secondly to provide Affective Computing solutions in order to evaluate emotions in User eXperience studies. Through a study using hand-gesture interface devices, three components of users\u0026rsquo; emotions were simultaneously measured with self-reports: the subjective, cognitive and motivational components. The results point out the possibility of measuring different components in order to gain a better understanding of emotions triggered by products. They also point out that self-reports measures could be improved with Affective Computing solutions. In this perspective, two emotion assessment tools were developed: Oudjat and EmoLyse.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"a31d6512260e0f4ad9a2dd62ee4b2b98","permalink":"/talk/2015-acii/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2015-acii/","section":"talk","summary":"ABSTRACT: User eXperience studies with products, systems or services have significantly increased in companies in order to anticipate their commercial success. Among the user experience dimensions, emotions are predominant. However User eXperience studies used several concepts to refer to emotions and current measures still have some flaws.","tags":null,"title":"Emotions triggered by innovative products, A multi-componential approach of emotions for User eXperience tools","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT:\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"060b6511ae3c46a0d81d65e321f8e8cf","permalink":"/talk/2016-aiptlf/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2016-aiptlf/","section":"talk","summary":"ABSTRACT:","tags":null,"title":"Etude de l’expérience utilisateur émotionnelle: Quels sont les impacts des émotions suscitées des produits innovants sur les perceptions des utilisateurs","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT: In supporting the visual-mediated emotional recognition, little research has centred on analysing the effect of presenting different combinatorial facial designs. Moreover, in the theoretical field of emotions, research on the recognition of emotional facial expressions (EFE) is mainly based on static and posed databases tested in unnatural contexts (explicit recognition task of an emotion). Our research has constructed and validated a database, DynEmo, with dynamic and spontaneous emotional facial expressions. So different facial interface designs (whole face, zoomed face, eyes + mouth, whole face + mouth, whole face + eyes, etc., n = 11) are experimentally compared to find their impact in terms of emotional recognition. The results show the facial interface design relevance in terms of emotional recognition. The application of this work is transferable to facial interfaces useful for video-mediated interaction.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"a57b931c51d65917e53b1642228dcafe","permalink":"/talk/2010-eiac/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2010-eiac/","section":"talk","summary":"ABSTRACT: In supporting the visual-mediated emotional recognition, little research has centred on analysing the effect of presenting different combinatorial facial designs. Moreover, in the theoretical field of emotions, research on the recognition of emotional facial expressions (EFE) is mainly based on static and posed databases tested in unnatural contexts (explicit recognition task of an emotion).","tags":null,"title":"Interface faciale émotionnelle : Les effets des différentes modalités de presentation","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT:\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"359f4ae252793bee69ab71b62da38dd8","permalink":"/talk/2012-de/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2012-de/","section":"talk","summary":"ABSTRACT:","tags":null,"title":"Measuring emotional states and behavioral responses to innovative products","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT: With the development of wearable sensors, it is now possible to assess the dynamic progression of physiological rhythms such as heart rate, breathing rate or galvanic skin response in ways and places and during certain activities that were previously not possible due to cost and reliability. This paper investigates the physiological changes when participating in a zipline activity. Despite the advances in sensor technology, the statistical analysis of such physiological signals remains a challenge for data analysts. This paper presents a workflow encompassing the whole process with the goal of obtaining a range of best fitting models to analyse the patterns given by these measurements “in the wild”.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"964a4c5a5dda77f0f2aaf0bdb0f7e5d1","permalink":"/talk/2018-mb/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2018-mb/","section":"talk","summary":"ABSTRACT: With the development of wearable sensors, it is now possible to assess the dynamic progression of physiological rhythms such as heart rate, breathing rate or galvanic skin response in ways and places and during certain activities that were previously not possible due to cost and reliability.","tags":null,"title":"Multivariate Body Area Network of Physiological Measures “In the Wild”: A case study with zipline activity","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT:\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"b2dbb3283c22fe9c4ed8e3a713f03ca4","permalink":"/talk/2010-cere/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2010-cere/","section":"talk","summary":"ABSTRACT:","tags":null,"title":"On-line recognition of dynamic and spontaneous facial expression","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT:\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"607b381a14579ed50fe8b980f8150a61","permalink":"/talk/2017-isre/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2017-isre/","section":"talk","summary":"ABSTRACT:","tags":null,"title":"Physiological correlates of Emotions “In The Wild”: A case study with mountain bikers","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT:\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"31b7bd8b8b67d213a167fcbbef50f7ed","permalink":"/talk/2014-wcfee/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2014-wcfee/","section":"talk","summary":"ABSTRACT:","tags":null,"title":"Spontaneous and dynamic emotional facial expressions reflect action readiness","type":"talk"},{"authors":null,"categories":null,"content":"ABSTRACT: Sharing personal information is an important way of communicating on social media. Among the information possibly shared, new sensors and tools allow people to share emotion information via facial emotion recognition. This paper questions whether people are prepared to share personal information such as their own emotion on social media. In the current study we examined how factors such as felt emotion, motivation for sharing on social media as well as personality affected participants’ willingness to share self-reported emotion or facial expression online. By carrying out a Generalized Linear Mixed Model analysis, this study found that participants’ willingness to share self-reported emotion and facial expressions was influenced by their personality traits and the motivation for sharing their emotion information that they were given. From our results we can conclude that the estimated level of privacy for certain emotional information, such as facial expression, is influenced by the motivation for sharing the information online.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"784e4c5f7518565841d04eb2cc6dff26","permalink":"/talk/2018-dsaa/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2018-dsaa/","section":"talk","summary":"ABSTRACT: Sharing personal information is an important way of communicating on social media. Among the information possibly shared, new sensors and tools allow people to share emotion information via facial emotion recognition.","tags":null,"title":"Willingness to Share Emotion Information on Social Media: Influence of Personality and Social Context","type":"talk"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Damien Dupré","Daniel Akpan","Elena Elias","Jean-Michel Adam","Brigite Meillon","Nicolas Bonnefond","Michel Dubois","Anna Tcherkassof"],"categories":null,"content":"","date":1446336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446336000,"objectID":"f0684dc6daaf4418b49af034e176b749","permalink":"/publication/2015-oudjat-ijhcs/","publishdate":"2015-11-01T00:00:00Z","relpermalink":"/publication/2015-oudjat-ijhcs/","section":"publication","summary":"This paper describes Oudjat, a new software for the manual annotation of stimuli involved in facial expressions of emotion (FEE) recognition experiments. Taking into account advantages and weaknesses of existing software and keeping in mind the different specifications needed for annotation experiments, Oudjat provides a tradeoff between existing tools. For investigators, it is an easy-to-configure interface to set up relevant behaviors to be annotated. For annotators, it is an easy-to-use interface. This tool can perform complex annotations procedures with multiple responses panels such as buttons, scales as Likert scales, and free labeling. Oudjat also allows to chain response panels or to conduct sequence marking annotations (i.e. two-steps temporal annotation). As it can be configured in any language, Oudjat is particularly suited for intercultural experiments. Four annotation procedures are presented to illustrate Oudjat possibilities with FEE annotation. Oudjat is an open source software available to the scientific community and can be obtained on request.","tags":null,"title":"Oudjat, a configurable and usable annotation tool for the study of emotional stimuli","type":"publication"},{"authors":["Damien Dupré","Michel Dubois","Anna Tcherkassof","Pascal Pizelle"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"059c9c31e357ea6ed5170847ab6ab49f","permalink":"/publication/2015-acceptabilite-innovatio/","publishdate":"2015-09-01T00:00:00Z","relpermalink":"/publication/2015-acceptabilite-innovatio/","section":"publication","summary":"Anticipating the success of a product is a key issue for manufacturers, particularly in the case of an innovative product. To reduce the risk of being rejected by its target users, the field of psychology has examined the determinants of the emotional user experience generated by a product. The emotion felt by the user is indeed a key factor that will influence the use of a product. This paper presents the different methods for measuring emotions. A literature review will compare the studies measuring the emotions of potential product users, the type of measures they use and their main results. This literature review reveals that despite the importance of measuring the emotions of users to predict the success of products, few of them are performed on real products on the one hand and with a hypothetical-deductive approach on the other. Although the factor structure of emotional user experience is not yet explored, this framework is relevant for understanding the role of users’ emotions in their adoption of innovative products.","tags":null,"title":"Cadres et méthodes d’analyse des émotions suscitées par des produits innovants : Une revue bibliographique","type":"publication"},{"authors":null,"categories":["R"],"content":"\rR Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\r\rIncluding Plots\rYou can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1))\rpie(\rc(280, 60, 20),\rc(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;),\rcol = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;),\rinit.angle = -50, border = NA\r)\r\rFigure 1: A fancy pie chart.\r\r\r","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"10065deaa3098b0da91b78b48d0efc71","permalink":"/post/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/post/2015-07-23-r-rmarkdown/","section":"post","summary":"R Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":["Anna Tcherkassof","Damien Dupré","Brigite Meillon","Nadine Mandran","Michel Dubois","Jean-Michel Adam"],"categories":null,"content":"","date":1383264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1383264000,"objectID":"5fe94f371cc15a1ea486a19f9c576749","permalink":"/publication/2013-dynemo-ijma/","publishdate":"2013-11-01T00:00:00Z","relpermalink":"/publication/2013-dynemo-ijma/","section":"publication","summary":"DynEmo is a database available to the scientific community (https://dynemo.upmf-grenoble.fr/). It contains dynamic and natural emotional facial expressions (EFEs) displaying subjective affective states rated by both the expresser and observers. Methodological and contextual information is provided for each expression. This multimodal corpus meets psychological, ethical, and technical criteria. It is quite large, containing two sets of 233 and 125 recordings of EFE of ordinary Caucasian people (ages 25 to 65, 182 females and 176 males) filmed in natural but standardized conditions. In the Set 1, EFE recordings are associated with the affective state of the expresser (self-reported after the emotion inducing task, using dimensional, action readiness, and emotional labels items). In the Set 2, EFE recordings are both associated with the affective state of the expresser and with the time line (continuous annotations) of observers' ratings of the emotions displayed throughout the recording. The time line allows any researcher interested in analysing non-verbal human behavior to segment the expressions into emotions.","tags":null,"title":"DynEmo: A video database of natural facial expressions of emotions","type":"publication"},{"authors":["Michel Dubois","Damien Dupré","Jean-Michel Adam","Anna Tcherkassof","Nadine Mandran","Brigite Meillon"],"categories":null,"content":"","date":1362096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1362096000,"objectID":"79e32558600c5d7635fb96d24cd7a99a","permalink":"/publication/2013-interface-jmui/","publishdate":"2013-03-01T00:00:00Z","relpermalink":"/publication/2013-interface-jmui/","section":"publication","summary":"The use of facial interfaces in distant communications highlights the relevance of emotional recognition. However researches on emotional facial expression (EFE) recognition are mainly based on static and posed stimuli and their results are not much transferable to daily interactions. The purpose of the present study is to compare emotional recognition of authentic EFEs with 11 different interface designs. A widget allowing participants both to recognize an emotion and to assess it on-line was used. Divided-face and compound-face interfaces are compared with a common full frontal interface. Analytic and descriptive on-line results reveal that some interfaces facilitate emotional recognition whereas others would decrease it. This study suggests that relevant interfaces could improve emotional recognition and thus facilitate distant communications.","tags":null,"title":"The influence of facial designs interfaces on dynamic emotional recognition","type":"publication"},{"authors":["Damien Dupré","Mathieu Loiseau","Hamad Salem","Philipe Dessus","Stephan Simonian"],"categories":null,"content":"","date":1351728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1351728000,"objectID":"c78336291ffe032fef2b23895aff8a3b","permalink":"/publication/2012-critere-re/","publishdate":"2012-11-01T00:00:00Z","relpermalink":"/publication/2012-critere-re/","section":"publication","summary":"","tags":null,"title":"Quelques critères d’utilisation d’un outil d’évaluation automatique de synthèses de cours à distance","type":"publication"},{"authors":["Philipe Dessus","S. Trausan-Matu","F. Wild","Damien Dupré","Mathieu Loiseau","T. Rebedea","V. Zampa"],"categories":null,"content":"","date":1320105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1320105600,"objectID":"cf72556bf7b02744c193325dd5cae0fe","permalink":"/publication/2011-environement-ds/","publishdate":"2011-11-01T00:00:00Z","relpermalink":"/publication/2011-environement-ds/","section":"publication","summary":"","tags":null,"title":"Un environnement personnel d'apprentissage évaluant des distances épistémiques et dialogiques","type":"publication"}]