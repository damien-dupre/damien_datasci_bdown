<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Damien DataSci Blog on Damien DataSci Blog</title>
    <link>/</link>
    <description>Recent content in Damien DataSci Blog on Damien DataSci Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0100</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>How to organise a wedding with R: google search API, google drive, web scraping and automatic emails</title>
      <link>/post/how-to-organise-a-wedding-with-r-google-search-api-google-drive-web-scraping-and-automatic-emails/</link>
      <pubDate>Tue, 13 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/how-to-organise-a-wedding-with-r-google-search-api-google-drive-web-scraping-and-automatic-emails/</guid>
      <description>


&lt;p&gt;xxxxxxxxxxxxxxxxxxxx&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Time series clustering with Dynamic Time Warping (Part 2)</title>
      <link>/post/time-series-clustering-with-dynamic-time-warping-part-2/</link>
      <pubDate>Thu, 27 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/time-series-clustering-with-dynamic-time-warping-part-2/</guid>
      <description>


&lt;p&gt;Like every good movies, my previous blog post “Time series clustering with Dynamic Time Warping” deserves a sequel. In this Part 2, I will have a look at the athletes’ training plan for a marathon. Because marathons are such a demanding performance, most of athletes have a specific training plan to follow in order to be prepared. Many different training plan can be found on the web such as &lt;a href=&#34;https://www.runireland.com/wp-content/uploads/2018/01/Training_for_marathon.pdf&#34;&gt;this one&lt;/a&gt; from the website www.runireland.com.&lt;/p&gt;
&lt;p&gt;In this blog post I will try to cluster different simulated athlete training plans with Dynamic Time Warping and some seasonality, states and power band extractions.&lt;/p&gt;
&lt;div id=&#34;list-of-packages-needed&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;List of packages needed&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data wrangling
library(dplyr) # data wrangling
library(tidyr) # datawrangling
# analysis
library(dtwclust) # dynamic time warpping
library(depmixS4) # Hidden Markov Model
library(WaveletComp) # Wavelet Analysis
# graphics
library(ggplot2) # grammar of graphics
library(ggdendro) # grammar of dendrograms
library(gtable) # plot organisation
library(grid) # plot organisation
library(gridExtra) # plot organisation&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data simulation&lt;/h2&gt;
&lt;p&gt;For this purpose, I will create a data frame of 20 athlete training plans with 10 of them with a random plan and 10 with a repeated pattern non synchronized on their date and intensity. The main variable is the distance they have ran of every day since 25 weeks (175 days) before the marathon.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;date_marathon &amp;lt;- as.Date(&amp;quot;2015-10-26&amp;quot;)
#
df &amp;lt;- NULL
# random training plan with runs from 5 to 40km with a high proability of non run days (between 25% and 75% depending on athletes)
for (i in 1:10){
  random_proba &amp;lt;- runif(8)
  random_proba &amp;lt;- random_proba/sum(random_proba)
  value &amp;lt;- base::sample(
    x = seq(from = 0, to = 40, by = 5), 
    size = 175, 
    replace = TRUE, 
    prob = c(runif(1, 0.25, 0.75),random_proba)
  )
  athlete &amp;lt;- paste0(&amp;quot;athlete_rand_&amp;quot;,i)
  new_df &amp;lt;- data.frame(athlete = athlete, value = value, rundate = seq.Date(date_marathon-175, date_marathon-1, by=&amp;quot;day&amp;quot;))
  df &amp;lt;- rbind(df,new_df)
}
# training plan with a reapeated pattern with can change according the weeks and with a different intensity according athletes
for (i in 11:20){
  value &amp;lt;- rep_len(
    x = c(rep(x = 0, sample(1:3, 1)),10,0,15,20,30)*runif(1, 0.5, 1.5),
    length.out = 175
  )
  athlete &amp;lt;- paste0(&amp;quot;athlete_plan_&amp;quot;,i)
  new_df &amp;lt;- data.frame(athlete = athlete, value = value, rundate = seq.Date(date_marathon-175, date_marathon-1, by=&amp;quot;day&amp;quot;))
  df &amp;lt;- rbind(df,new_df)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have the data generated, a key trick will be to convert this data frame into a list of time series. The reason behind this choice is the possibility to implement a multivariate DTW analysis in the future (maybe in a Part 3).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plan_list &amp;lt;- df %&amp;gt;% 
  tidyr::spread(athlete,value) %&amp;gt;%
  dplyr::select(-rundate) %&amp;gt;%
  purrr::map(~(.))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dtw-cluster-on-raw-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;DTW cluster on raw data&lt;/h1&gt;
&lt;p&gt;After creating the list of data, let’s implement a simple DTW clustering on the raw data to see if we can identify our two groups.&lt;/p&gt;
&lt;div id=&#34;dtw-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;DTW model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Nclust &amp;lt;- 2
dtw_model &amp;lt;- dtwclust::tsclust(series = plan_list, 
                               type = &amp;quot;h&amp;quot;, 
                               k = Nclust,  
                               distance = &amp;quot;dtw_basic&amp;quot;, 
                               control = hierarchical_control(method = &amp;quot;complete&amp;quot;),
                               preproc = NULL, 
                               #args = tsclust_args(dist = list(window.size = 5L)),
                               trace = TRUE)
#
dtw_data &amp;lt;- ggdendro::dendro_data(dtw_model, type=&amp;quot;rectangle&amp;quot;)
#
labels_order &amp;lt;- dtw_data$labels$label
#
dtw_result &amp;lt;- data.frame(label = names(plan_list), 
                         cluster = factor(stats::cutree(dtw_model, k = Nclust)))
#
dtw_data[[&amp;quot;labels&amp;quot;]] &amp;lt;- merge(dtw_data[[&amp;quot;labels&amp;quot;]], dtw_result, by=&amp;quot;label&amp;quot;)
dtw_result &amp;lt;- dplyr::full_join(dtw_result,dtw_data$labels, by=c(&amp;quot;label&amp;quot;, &amp;quot;cluster&amp;quot;))%&amp;gt;%
  dplyr::arrange(x)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dtw-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;DTW plot&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cluster_box &amp;lt;- aggregate(x~cluster, ggdendro::label(dtw_data), range)
cluster_box &amp;lt;- data.frame(cluster_box$cluster,cluster_box$x)
cluster_threshold &amp;lt;- mean(dtw_model$height[length(dtw_model$height)-((Nclust-2):(Nclust-1))])
#
numColors &amp;lt;- length(levels(dtw_result$cluster)) # How many colors you need
getColors &amp;lt;- scales::hue_pal() # Create a function that takes a number and returns a qualitative palette of that length (from the scales package)
myPalette &amp;lt;- getColors(numColors)
names(myPalette) &amp;lt;- levels(dtw_result$cluster) # Give every color an appropriate name

p1 &amp;lt;- ggplot() + 
  geom_rect(data=cluster_box, aes(xmin=X1-.3, xmax=X2+.3, ymin=0, ymax=cluster_threshold, color=cluster_box.cluster), fill=NA)+
  geom_segment(data=ggdendro::segment(dtw_data), aes(x=x, y=y, xend=xend, yend=yend)) + 
  coord_flip() + 
  scale_y_continuous(&amp;quot;Distance&amp;quot;) + 
  scale_x_continuous(&amp;quot;&amp;quot;,breaks = 1:20, labels = labels_order) + 
  guides(color=FALSE, fill = FALSE)+
  theme(
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank(), # remove grids
    panel.background = element_blank(), 
    axis.text.y = element_text(colour = myPalette[dtw_result$cluster],hjust=0.5),
    axis.ticks.y=element_blank()
  )
#
p2 &amp;lt;- as.data.frame(matrix(unlist(plan_list), 
                           nrow=length(unlist(plan_list[1])), 
                           dimnames = list(c(),names(plan_list)))) %&amp;gt;%
  dplyr::mutate(rundatelocal = seq.Date(date_marathon-175, date_marathon-1, by=&amp;quot;day&amp;quot;)) %&amp;gt;%
  tidyr::gather(key = label,value = value, -rundatelocal) %&amp;gt;%
  dplyr::mutate(label = as.factor(label)) %&amp;gt;%
  dplyr::full_join(., dtw_result, by = &amp;quot;label&amp;quot;) %&amp;gt;% 
  mutate(label = factor(label, levels = rev(as.character(labels_order)))) %&amp;gt;%
  ggplot(aes(x = rundatelocal, y = value, colour = as.factor(cluster))) +
  geom_line() +
  geom_area(aes(fill = as.factor(cluster))) +
  coord_cartesian(ylim = c(0,50)) +
  scale_y_continuous(name = &amp;quot;Total distance per day [km]&amp;quot;, breaks = seq(0, 50, by = 50)) +
  scale_x_date(name = &amp;quot;Run Date&amp;quot;, date_breaks = &amp;quot;4 week&amp;quot;, date_labels = &amp;quot;%b %d&amp;quot;) +
  facet_wrap(~label, ncol = 1, strip.position=&amp;quot;left&amp;quot;) + 
  guides(color=FALSE, fill = FALSE) +
  theme_bw() + 
  theme(strip.background = element_blank(), strip.text = element_blank())
#
plt_list &amp;lt;- list(p2, p1)
plt_layout &amp;lt;- rbind(c(NA, 2),
                    c(1, 2),
                    c(NA, 2))
#
grid.arrange(grobs = plt_list, layout_matrix = plt_layout, heights = c(0.04, 1, 0.05))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-27-time-series-clustering-with-dynamic-time-warping-part-2_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I think I get a nice plot thanks to the solutions provided on Stack Overflow graphically speaking (except some overlap with the labels but I’m working on it). The results are not too bad but some of the random plan can be included in the repeated pattern plan. Well randomness can be expected and can create some nice patterns sometimes. Another interesting result is the necessity to increase the cluster number in order to have a clean clustering.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;centroids&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Centroids&lt;/h3&gt;
&lt;p&gt;We can also have a look at the centroids to see with plans are the most representative of the clusters. Obviously with only two clusters, it is not very useful but it can be a key element to distinguish between many different training plans.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dtw_model_centroids &amp;lt;- data.frame(dtw_model@centroids, rundatelocal = seq.Date(date_marathon-175, date_marathon-1, by=&amp;quot;day&amp;quot;)) %&amp;gt;%
  tidyr::gather(label, totaldistancekm, starts_with(&amp;quot;athlete&amp;quot;)) %&amp;gt;%
  dplyr::left_join(., dtw_result, by = &amp;quot;label&amp;quot;) %&amp;gt;% 
  dplyr::mutate(label = factor(label, levels = rev(labels_order)))
#
dtw_model_centroids %&amp;gt;%
  ggplot(aes(rundatelocal,totaldistancekm, color = cluster, fill = cluster)) +
  geom_line() +
  geom_area() +
  facet_wrap(~ label + cluster, ncol = 1, strip.position=&amp;quot;right&amp;quot;, labeller=labeller(.rows = label_both)) +
  scale_y_continuous(name = &amp;quot;Total distance per day [km]&amp;quot;) +
  scale_x_date(name = &amp;quot;Run Date&amp;quot;, date_breaks = &amp;quot;4 week&amp;quot;, date_labels = &amp;quot;%b %d&amp;quot;) +
  guides(color=FALSE, fill = FALSE) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-27-time-series-clustering-with-dynamic-time-warping-part-2_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The main problem with raw data is the noise. In order to extract recurrent patterns, the randomness of the noise can sometimes simulate non meaningful pattern and then change the cluster structure. Because we are interested in classification of recurrent pattern, a nice thing would be to remove the noise. Noise removal analyses are probably the most important contribution of signal treatment research and many can be applied here such as seasonality decomposition, Hidden Markov Models and power spectrum analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dtw-cluster-with-seasonality-decomposition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;DTW cluster with seasonality decomposition&lt;/h2&gt;
&lt;p&gt;R for time series analysis have some unavoidable packages and functions. If you are interested in time series analysis, you probably cannot work without &lt;code&gt;zoo::zoo()&lt;/code&gt;, &lt;code&gt;xts::xts()&lt;/code&gt; or &lt;code&gt;tibbletime::as_tbl_time()&lt;/code&gt;. However for time series analysis, the &lt;code&gt;stats&lt;/code&gt; package have one of the most used and nice function: &lt;code&gt;stl()&lt;/code&gt;. Stl allows a Seasonal Decomposition of Time Series by Loess which is powerful in order to extract time series noise, trend and seasonality (i.e periods). In our case we will try to use &lt;code&gt;stl()&lt;/code&gt; in order to extract training plan seasonality over one week and then to cluster the results with the DTW method.&lt;/p&gt;
&lt;p&gt;So first let’s apply the &lt;code&gt;stl()&lt;/code&gt; decomposition to every time series in our master list.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_seasonality &amp;lt;- function(x, robust){
  x_ts = ts(as.numeric(unlist(x)), frequency = 7)
  stl_test = stl(x_ts, s.window = 7, robust)
  return(stl_test$time.series[,1])
}
#
plan_seasonality &amp;lt;- plan_list %&amp;gt;%
  purrr::map(~extract_seasonality(., robust = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then let’s process our model and to plot the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Nclust &amp;lt;- 2
dtw_model &amp;lt;- dtwclust::tsclust(series = plan_seasonality, 
                               type = &amp;quot;h&amp;quot;, 
                               k = Nclust,  
                               distance = &amp;quot;dtw_basic&amp;quot;, 
                               control = hierarchical_control(method = &amp;quot;complete&amp;quot;),
                               preproc = NULL, 
                               #args = tsclust_args(dist = list(window.size = 5L)),
                               trace = TRUE)
#
dtw_data &amp;lt;- ggdendro::dendro_data(dtw_model, type=&amp;quot;rectangle&amp;quot;)
#
labels_order &amp;lt;- dtw_data$labels$label
#
dtw_result &amp;lt;- data.frame(label = names(plan_seasonality), 
                         cluster = factor(stats::cutree(dtw_model, k = Nclust)))
#
dtw_data[[&amp;quot;labels&amp;quot;]] &amp;lt;- merge(dtw_data[[&amp;quot;labels&amp;quot;]], dtw_result, by=&amp;quot;label&amp;quot;)
dtw_result &amp;lt;- dplyr::full_join(dtw_result,dtw_data$labels, by=c(&amp;quot;label&amp;quot;, &amp;quot;cluster&amp;quot;))%&amp;gt;%
  dplyr::arrange(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cluster_box &amp;lt;- aggregate(x~cluster, ggdendro::label(dtw_data), range)
cluster_box &amp;lt;- data.frame(cluster_box$cluster,cluster_box$x)
cluster_threshold &amp;lt;- mean(dtw_model$height[length(dtw_model$height)-((Nclust-2):(Nclust-1))])
#
numColors &amp;lt;- length(levels(dtw_result$cluster)) # How many colors you need
getColors &amp;lt;- scales::hue_pal() # Create a function that takes a number and returns a qualitative palette of that length (from the scales package)
myPalette &amp;lt;- getColors(numColors)
names(myPalette) &amp;lt;- levels(dtw_result$cluster) # Give every color an appropriate name

p1 &amp;lt;- ggplot() + 
  geom_rect(data=cluster_box, aes(xmin=X1-.3, xmax=X2+.3, ymin=0, ymax=cluster_threshold, color=cluster_box.cluster), fill=NA)+
  geom_segment(data=ggdendro::segment(dtw_data), aes(x=x, y=y, xend=xend, yend=yend)) + 
  coord_flip() + 
  scale_y_continuous(&amp;quot;Distance&amp;quot;) + 
  scale_x_continuous(&amp;quot;&amp;quot;,breaks = 1:20, labels = labels_order) + 
  guides(color=FALSE, fill = FALSE)+
  theme(
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank(), # remove grids
    panel.background = element_blank(), 
    axis.text.y = element_text(colour = myPalette[dtw_result$cluster],hjust=0.5),
    axis.ticks.y=element_blank()
  )
#
p2 &amp;lt;- as.data.frame(matrix(unlist(plan_seasonality), 
                           nrow=length(unlist(plan_seasonality[1])), 
                           dimnames = list(c(),names(plan_seasonality)))) %&amp;gt;%
  dplyr::mutate(rundatelocal = seq.Date(date_marathon-175, date_marathon-1, by=&amp;quot;day&amp;quot;)) %&amp;gt;%
  tidyr::gather(key = label,value = value, -rundatelocal) %&amp;gt;%
  dplyr::mutate(label = as.factor(label)) %&amp;gt;%
  dplyr::full_join(., dtw_result, by = &amp;quot;label&amp;quot;) %&amp;gt;% 
  mutate(label = factor(label, levels = rev(as.character(labels_order)))) %&amp;gt;%
  ggplot(aes(x = rundatelocal, y = value, colour = as.factor(cluster))) +
  geom_line() +
  geom_area(aes(fill = as.factor(cluster))) +
  coord_cartesian(ylim = c(-25,25)) +
  scale_y_continuous(name = &amp;quot;Seasonal distance per day [km]&amp;quot;, breaks = seq(-25, 25, by = 50)) +
  scale_x_date(name = &amp;quot;Run Date&amp;quot;, date_breaks = &amp;quot;4 week&amp;quot;, date_labels = &amp;quot;%b %d&amp;quot;) +
  facet_wrap(~label, ncol = 1, strip.position=&amp;quot;left&amp;quot;) + 
  guides(color=FALSE, fill = FALSE) +
  theme_bw() + 
  theme(strip.background = element_blank(), strip.text = element_blank())
#
plt_list &amp;lt;- list(p2, p1)
plt_layout &amp;lt;- rbind(c(NA, 2),
                    c(1, 2),
                    c(NA, 2))
#
grid.arrange(grobs = plt_list, layout_matrix = plt_layout, heights = c(0.04, 1, 0.05))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-27-time-series-clustering-with-dynamic-time-warping-part-2_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well that’s an epic fail I think but let’s have a look why. Different reasons can explain why we have a first cluster with only 3 time series and as second with all the 17 remaining ones:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;I am using only 2 clusters. In the real life (and in real randomness) a large amount of pattern is possible thus increasing the number of clusters can make the clustering more efficient (if an evaluation of the best fit with cluster number is performed).&lt;/li&gt;
&lt;li&gt;By removing the noise in the random plan, I made them not random at all and we can see now the repetition of the patterns. This is exactly what I want in my research with real data but here it made a mess.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So let’s try another method!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dtw-cluster-with-hidden-markov-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;DTW cluster with Hidden Markov Model&lt;/h2&gt;
&lt;p&gt;I’m not a perfect expert in Hidden Markov Model (HMM) and after having a look at the book &lt;a href=&#34;https://www.crcpress.com/p/book/9781482253832&#34;&gt;Hidden Markov Models for Time Series An Introduction Using R&lt;/a&gt; by Walter Zucchini, Iain L. MacDonald and Roland Langrock, I can surely say that it is a complicated question. However in a nutshell HMM are clustering the values according their probability to be part of a “state”. In our case, let’s say that we have three states possible per day “no run”, “medium run” and “long run”. By using HMM it is possible to create new time series based on states instead on distance. It’s a qualitative transformation without any prior assumption about the states boundaries (almost).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plan_HMM &amp;lt;- as.data.frame(matrix(unlist(plan_list), 
                           nrow=length(unlist(plan_list[1])), 
                           dimnames = list(c(),names(plan_list)))) %&amp;gt;%
  dplyr::mutate(rundatelocal = seq.Date(date_marathon-175, date_marathon-1, by=&amp;quot;day&amp;quot;)) %&amp;gt;%
  tidyr::gather(key = label,value = value, -rundatelocal) %&amp;gt;%
  dplyr::mutate(label = as.factor(label)) %&amp;gt;%
  dplyr::mutate(value = as.integer(value))
#
mod &amp;lt;- depmixS4::depmix(value~label, family = poisson(link = &amp;quot;log&amp;quot;), nstates = 3, data = plan_HMM)
#
fm  &amp;lt;- depmixS4::fit(mod, verbose = FALSE)
#
probs &amp;lt;- depmixS4::posterior(fm)
#
plan_HMM &amp;lt;- cbind(plan_HMM,probs) %&amp;gt;%
  dplyr::select(rundatelocal,label,state) %&amp;gt;%
  tidyr::spread(label,state) %&amp;gt;%
  dplyr::select(-rundatelocal) %&amp;gt;%
  purrr::map(~(.))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Nclust &amp;lt;- 2
dtw_model &amp;lt;- dtwclust::tsclust(series = plan_HMM, 
                               type = &amp;quot;h&amp;quot;, 
                               k = Nclust,  
                               distance = &amp;quot;dtw_basic&amp;quot;, 
                               control = hierarchical_control(method = &amp;quot;complete&amp;quot;),
                               preproc = NULL, 
                               #args = tsclust_args(dist = list(window.size = 5L)),
                               trace = TRUE)
#
dtw_data &amp;lt;- ggdendro::dendro_data(dtw_model, type=&amp;quot;rectangle&amp;quot;)
#
labels_order &amp;lt;- dtw_data$labels$label
#
dtw_result &amp;lt;- data.frame(label = names(plan_HMM), 
                         cluster = factor(stats::cutree(dtw_model, k = Nclust)))
#
dtw_data[[&amp;quot;labels&amp;quot;]] &amp;lt;- merge(dtw_data[[&amp;quot;labels&amp;quot;]], dtw_result, by=&amp;quot;label&amp;quot;)
dtw_result &amp;lt;- dplyr::full_join(dtw_result,dtw_data$labels, by=c(&amp;quot;label&amp;quot;, &amp;quot;cluster&amp;quot;))%&amp;gt;%
  dplyr::arrange(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cluster_box &amp;lt;- aggregate(x~cluster, ggdendro::label(dtw_data), range)
cluster_box &amp;lt;- data.frame(cluster_box$cluster,cluster_box$x)
cluster_threshold &amp;lt;- mean(dtw_model$height[length(dtw_model$height)-((Nclust-2):(Nclust-1))])
#
numColors &amp;lt;- length(levels(dtw_result$cluster)) # How many colors you need
getColors &amp;lt;- scales::hue_pal() # Create a function that takes a number and returns a qualitative palette of that length (from the scales package)
myPalette &amp;lt;- getColors(numColors)
names(myPalette) &amp;lt;- levels(dtw_result$cluster) # Give every color an appropriate name

p1 &amp;lt;- ggplot() + 
  geom_rect(data=cluster_box, aes(xmin=X1-.3, xmax=X2+.3, ymin=0, ymax=cluster_threshold, color=cluster_box.cluster), fill=NA)+
  geom_segment(data=ggdendro::segment(dtw_data), aes(x=x, y=y, xend=xend, yend=yend)) + 
  coord_flip() + 
  scale_y_continuous(&amp;quot;Distance&amp;quot;) + 
  scale_x_continuous(&amp;quot;&amp;quot;,breaks = 1:20, labels = labels_order) + 
  guides(color=FALSE, fill = FALSE)+
  theme(
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank(), # remove grids
    panel.background = element_blank(), 
    axis.text.y = element_text(colour = myPalette[dtw_result$cluster],hjust=0.5),
    axis.ticks.y=element_blank()
  )
#
p2 &amp;lt;- as.data.frame(matrix(unlist(plan_HMM), 
                           nrow=length(unlist(plan_HMM[1])), 
                           dimnames = list(c(),names(plan_HMM)))) %&amp;gt;%
  dplyr::mutate(rundatelocal = seq.Date(date_marathon-175, date_marathon-1, by=&amp;quot;day&amp;quot;)) %&amp;gt;%
  tidyr::gather(key = label,value = value, -rundatelocal) %&amp;gt;%
  dplyr::mutate(label = as.factor(label)) %&amp;gt;%
  dplyr::full_join(., dtw_result, by = &amp;quot;label&amp;quot;) %&amp;gt;% 
  mutate(label = factor(label, levels = rev(as.character(labels_order)))) %&amp;gt;%
  ggplot(aes(x = rundatelocal, y = value, colour = as.factor(cluster))) +
  geom_line() +
  geom_area(aes(fill = as.factor(cluster))) +
  coord_cartesian(ylim = c(0,4)) +
  scale_y_continuous(name = &amp;quot;States per day [km]&amp;quot;, breaks = seq(0, 4, by = 4)) +
  scale_x_date(name = &amp;quot;Run Date&amp;quot;, date_breaks = &amp;quot;4 week&amp;quot;, date_labels = &amp;quot;%b %d&amp;quot;) +
  facet_wrap(~label, ncol = 1, strip.position=&amp;quot;left&amp;quot;) + 
  guides(color=FALSE, fill = FALSE) +
  theme_bw() + 
  theme(strip.background = element_blank(), strip.text = element_blank())
#
plt_list &amp;lt;- list(p2, p1)
plt_layout &amp;lt;- rbind(c(NA, 2),
                    c(1, 2),
                    c(NA, 2))
#
grid.arrange(grobs = plt_list, layout_matrix = plt_layout, heights = c(0.04, 1, 0.05))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-27-time-series-clustering-with-dynamic-time-warping-part-2_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Good news this time, the clusters are almost equally distributed, bad news random plans and pattern plans are mixed together. However we can see that the HMM is creating surprisingly nice pattern which can be easily clustered with a higher number of cluster. The drawback is the low distance between each time series which can make the clustering method more complicated.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dtw-cluster-by-power-spectral-density&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;DTW cluster by power spectral density&lt;/h2&gt;
&lt;p&gt;Last but not least, the probable best approach to evaluate seasonality/frequency in training plan pattern can be the power spectrum analysis. By identifying the underlying frequencies of each time series it is possible to cluster them according their pattern. A nice new package &lt;code&gt;WaveletComp&lt;/code&gt; can be used for this purpose. &lt;code&gt;WaveletComp&lt;/code&gt; is analyzing the frequency structure of uni- and bivariate time series using the Morlet wavelet.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_poweravg &amp;lt;- function(x){
  x &amp;lt;- as.data.frame(x)
  power_spectrum &amp;lt;- WaveletComp::analyze.wavelet(
    my.data = x,
    my.series = 1,
    loess.span = 0,
    dt = 1,
    verbose = FALSE
  )
  max_period &amp;lt;- max(power_spectrum$Period)
  dat &amp;lt;- spline(power_spectrum$Power.avg, n = max_period)$y # WARNING:power band starts at 2 not 1
  return(dat)
}
plan_poweravge &amp;lt;- plan_list %&amp;gt;%
  purrr::map(~extract_poweravg(.))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Nclust &amp;lt;- 2
dtw_model &amp;lt;- dtwclust::tsclust(series = plan_poweravge, 
                               type = &amp;quot;h&amp;quot;, 
                               k = Nclust,  
                               distance = &amp;quot;dtw_basic&amp;quot;, 
                               control = hierarchical_control(method = &amp;quot;complete&amp;quot;),
                               preproc = NULL, 
                               #args = tsclust_args(dist = list(window.size = 5L)),
                               trace = TRUE)
#

dtw_data &amp;lt;- ggdendro::dendro_data(dtw_model, type=&amp;quot;rectangle&amp;quot;)
#
labels_order &amp;lt;- dtw_data$labels$label
#
dtw_result &amp;lt;- data.frame(label = names(plan_poweravge), 
                         cluster = factor(stats::cutree(dtw_model, k = Nclust)))
#
dtw_data[[&amp;quot;labels&amp;quot;]] &amp;lt;- merge(dtw_data[[&amp;quot;labels&amp;quot;]], dtw_result, by=&amp;quot;label&amp;quot;)
dtw_result &amp;lt;- dplyr::full_join(dtw_result,dtw_data$labels, by=c(&amp;quot;label&amp;quot;, &amp;quot;cluster&amp;quot;))%&amp;gt;%
  dplyr::arrange(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cluster_box &amp;lt;- aggregate(x~cluster, ggdendro::label(dtw_data), range)
cluster_box &amp;lt;- data.frame(cluster_box$cluster,cluster_box$x)
cluster_threshold &amp;lt;- mean(dtw_model$height[length(dtw_model$height)-((Nclust-2):(Nclust-1))])
#
numColors &amp;lt;- length(levels(dtw_result$cluster)) # How many colors you need
getColors &amp;lt;- scales::hue_pal() # Create a function that takes a number and returns a qualitative palette of that length (from the scales package)
myPalette &amp;lt;- getColors(numColors)
names(myPalette) &amp;lt;- levels(dtw_result$cluster) # Give every color an appropriate name

p1 &amp;lt;- ggplot() + 
  geom_rect(data=cluster_box, aes(xmin=X1-.3, xmax=X2+.3, ymin=0, ymax=cluster_threshold, color=cluster_box.cluster), fill=NA)+
  geom_segment(data=ggdendro::segment(dtw_data), aes(x=x, y=y, xend=xend, yend=yend)) + 
  coord_flip() + 
  scale_y_continuous(&amp;quot;Distance&amp;quot;) + 
  scale_x_continuous(&amp;quot;&amp;quot;,breaks = 1:20, labels = labels_order) + 
  guides(color=FALSE, fill = FALSE)+
  theme(
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank(), # remove grids
    panel.background = element_blank(), 
    axis.text.y = element_text(colour = myPalette[dtw_result$cluster],hjust=0.5),
    axis.ticks.y=element_blank()
  )
#
p2 &amp;lt;- as.data.frame(matrix(unlist(plan_poweravge), 
                           nrow=length(unlist(plan_poweravge[1])), 
                           dimnames = list(c(),names(plan_poweravge)))) %&amp;gt;%
  dplyr::mutate(rundatelocal = 1:n()) %&amp;gt;%
  tidyr::gather(key = label,value = value, -rundatelocal) %&amp;gt;%
  dplyr::mutate(label = as.factor(label)) %&amp;gt;%
  dplyr::full_join(., dtw_result, by = &amp;quot;label&amp;quot;) %&amp;gt;% 
  mutate(label = factor(label, levels = rev(as.character(labels_order)))) %&amp;gt;%
  ggplot(aes(x = rundatelocal, y = value, colour = as.factor(cluster))) +
  geom_line() +
  geom_area(aes(fill = as.factor(cluster))) +
  coord_cartesian(ylim = c(0,1)) +
  scale_y_continuous(name = &amp;quot;Average power density&amp;quot;, breaks = seq(0, 1, by = 1)) +
  scale_x_continuous(name = &amp;quot;Period (days)&amp;quot;) +
  facet_wrap(~label, ncol = 1, strip.position=&amp;quot;left&amp;quot;) + 
  guides(color=FALSE, fill = FALSE) +
  theme_bw() + 
  theme(strip.background = element_blank(), strip.text = element_blank())
#
plt_list &amp;lt;- list(p2, p1)
plt_layout &amp;lt;- rbind(c(NA, 2),
                    c(1, 2),
                    c(NA, 2))
#
grid.arrange(grobs = plt_list, layout_matrix = plt_layout, heights = c(0.04, 1, 0.05))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-27-time-series-clustering-with-dynamic-time-warping-part-2_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This frequency decomposition looks amazing but be careful because the power frequency are average and as stated in &lt;a href=&#34;http://www.hs-stat.com/projects/WaveletComp/WaveletComp_guided_tour.pdf&#34;&gt;“WaveletComp 1.1:A guided tour through the R package”&lt;/a&gt;, “The average power plot cannot distinguish between consecutive periods and overlapping periods”. This is annoying but average power is definitely a first step toward a nice classification of training plan patterns.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A simple introduction to statistical test and statistical significance</title>
      <link>/post/a-simple-introduction-to-statistical-test-and-statistical-significance/</link>
      <pubDate>Thu, 13 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-simple-introduction-to-statistical-test-and-statistical-significance/</guid>
      <description>


&lt;p&gt;Recently I was asked to make a short presentation about how to introduce the idea of statistical test and statistical significance to students in marketing. The timing was very good because I just had red a blog post written by Stephen B. Heard called &lt;a href=&#34;https://scientistseessquirrel.wordpress.com/2015/10/06/why-do-we-make-statistics-so-hard-for-our-students/&#34;&gt;Why do we make statistics so hard for our students?&lt;/a&gt; which is exactly the way to follow. So I taken the main idea of this post and made a short presentation with R that you can find here:&lt;/p&gt;
&lt;iframe src=&#34;https://damien-datasci-blog.netlify.com/slides/DCU_presentation&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;I’ve also taken a simple meme which looks very trendy on the net. A great sentence by W. Edwards Deming “Without data you’re just another person with an opinion”. It’s simple but it is the reason why Statistics are very important not only in academia but also in companies.&lt;/p&gt;
&lt;p&gt;PS: Special thanks to Tim Mastny for his blog post about &lt;a href=&#34;https://timmastny.rbind.io/blog/embed-slides-knitr-blogdown/&#34;&gt;how to host R presentation in a blogdown&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Time series clustering with Dynamic Time Warping</title>
      <link>/post/time-series-clustering-with-dynamic-time-warp/</link>
      <pubDate>Thu, 06 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/time-series-clustering-with-dynamic-time-warp/</guid>
      <description>


&lt;p&gt;Many solutions for clustering time series are available with R and as usual the web is full of nice tutorials like &lt;a href=&#34;http://girke.bioinformatics.ucr.edu/GEN242/pages/mydoc/Rclustering.html&#34;&gt;Thomas Girke’s blog post&lt;/a&gt;, &lt;a href=&#34;http://genomicsclass.github.io/book/pages/clustering_and_heatmaps.html&#34;&gt;Rafael Irizarry and Michael Love’s book&lt;/a&gt;, &lt;a href=&#34;https://datawookie.netlify.com/blog/2017/04/clustering-time-series-data/&#34;&gt;Andrew B. Collier’s blog post&lt;/a&gt;, &lt;a href=&#34;https://petolau.github.io/TSrepr-clustering-time-series-representations/&#34;&gt;Peter Laurinec’s blog post&lt;/a&gt;, &lt;a href=&#34;http://www.stat.unc.edu/faculty/pipiras/timeseries/Multivariate_6_-_Classification_Clustering_-_Menu.html&#34;&gt;Dylan Glotzer’s lecture&lt;/a&gt; or &lt;a href=&#34;http://rstudio-pubs-static.s3.amazonaws.com/398402_abe1a0343a4e4e03977de8f3791e96bb.html&#34;&gt;Ana Rita Marques’s module&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Dynamic Time Warping (DTW) is one of these solutions. The main advantage of DTW is the possibility to group time series according their patterns or shapes even if these patterns are not synchronized (lag).&lt;/p&gt;
&lt;p&gt;As far as I know the two main packages which allow time series clustering with DTW are &lt;code&gt;TSclust&lt;/code&gt; by &lt;a href=&#34;https://cran.r-project.org/web/packages/TSclust/index.html&#34;&gt;Pablo Montero Manso and José Antonio Vilar&lt;/a&gt; and &lt;code&gt;dtwclust&lt;/code&gt; by &lt;a href=&#34;https://cran.r-project.org/web/packages/dtwclust/index.html&#34;&gt;Alexis Sarda-Espinosa&lt;/a&gt;. These packages are very simple but powerful tools to analyse time series. However when it comes to analyse real data, I found difficult to understand how the clustering is working. To make this process clearer I’m going to simulate two groups of time series and to check if whether or not the DTW clustering can differentiate them.&lt;/p&gt;
&lt;div id=&#34;list-of-packages-needed&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;List of packages needed&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr) # data wrangling
library(ggplot2) # grammar of graphics
library(gridExtra) # merge plots
library(ggdendro) # dendrograms
library(gplots) # heatmap
library(tseries) # bootstrap
library(TSclust) # cluster time series
library(dtwclust) # cluster time series with dynamic time warping&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data simulation&lt;/h2&gt;
&lt;p&gt;Let’s imagine two people running a marathon, one had a classic run with a pace increasing with the time and the other had a very bad experience (e.g. “hitting the wall”) with a jump in the pace which indicates a significant slow down in the second part of the run. The best is to have real data to analyse but it can be very useful to simulate these pattern in order to assess the clustering efficiency.&lt;/p&gt;
&lt;p&gt;A simple way to simulate these time series is to use the &lt;code&gt;sine&lt;/code&gt; function and to add a random noise in order to make it more credible.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# classic run
noise &amp;lt;- runif(420) # random noise
x &amp;lt;- seq(1,420) # 42km with a measure every 100m
pace_min &amp;lt;- 5 # min/km (corresponds to fast run)

ts_sim_classic_run &amp;lt;- (sin(x/10)+x/100+noise+pace_min) %&amp;gt;%
  as.ts(.)

ts.plot(ts_sim_classic_run, xlab = &amp;quot;Distance [x100m]&amp;quot;, ylab = &amp;quot;Differential pace [min/km]&amp;quot;, main = &amp;quot;Example of classic run&amp;quot;, ylim=c(0,25))

# wall run
noise &amp;lt;- runif(210) # random noise
x &amp;lt;- seq(1,210) # 21km with a measure every 100m 
pace_min &amp;lt;- 5 # min/km (corresponds to fast run)
pace_wall &amp;lt;- 20 # min/km (corresponds to very slow run) 
ts_sim_part1 &amp;lt;- sin(x/5)+x/50+noise+pace_min
ts_sim_part2 &amp;lt;- sin(x/5)+noise+pace_wall

ts_sim_wall_run &amp;lt;- c(ts_sim_part1,ts_sim_part2) %&amp;gt;%
  as.ts(.)

ts.plot(ts_sim_wall_run, xlab = &amp;quot;Distance [x100m]&amp;quot;, ylab = &amp;quot;Differential pace [min/km]&amp;quot;, main = &amp;quot;Example of wall run&amp;quot;, ylim=c(0,25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-06-time-series-clustering-with-dynamic-time-warp_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;384&#34; /&gt;&lt;img src=&#34;/post/2018-09-06-time-series-clustering-with-dynamic-time-warp_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A much nicer way would be to use ARIMA with an auto regressive model (AR).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pace_min &amp;lt;- 5 # min/km (corresponds to fast run)
pace_wall &amp;lt;- 20 # min/km (corresponds to very slow run) 

# classic run
ts_sim_classic_run &amp;lt;- abs(arima.sim(n = 420, mean = 0.001, model = list(order = c(1,0,0), ar = 0.9))) + pace_min

ts.plot(ts_sim_classic_run, xlab = &amp;quot;Distance [x100m]&amp;quot;, ylab = &amp;quot;Differential pace [min/km]&amp;quot;, main = &amp;quot;Example of classic run&amp;quot;, ylim=c(0,25))

# wall run
ts_sim_part1 &amp;lt;- abs(arima.sim(n = 210, model = list(order = c(1,0,0), ar = 0.9))) + pace_min
ts_sim_part2 &amp;lt;- ts(arima.sim(n = 210, model = list(order = c(1,0,0), ar = 0.9)) + pace_wall, start = 211,end =420)

ts_sim_wall_run &amp;lt;- ts.union(ts_sim_part1,ts_sim_part2)
ts_sim_wall_run&amp;lt;- pmin(ts_sim_wall_run[,1], ts_sim_wall_run[,2], na.rm = TRUE)

ts.plot(ts_sim_wall_run, xlab = &amp;quot;Distance [x100m]&amp;quot;, ylab = &amp;quot;Differential pace [min/km]&amp;quot;, main = &amp;quot;Example of wall run&amp;quot;, ylim=c(0,25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-06-time-series-clustering-with-dynamic-time-warp_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;384&#34; /&gt;&lt;img src=&#34;/post/2018-09-06-time-series-clustering-with-dynamic-time-warp_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bootstrap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bootstrap&lt;/h2&gt;
&lt;p&gt;Now we have two different runs, let’s bootstrap them (i.e. replicate with small differences) in order to have two groups of 5 individuals for each run type.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ts_sim_boot_classic &amp;lt;- ts_sim_classic_run %&amp;gt;%
  tseries::tsbootstrap(., nb=5, b=200, type = &amp;quot;block&amp;quot;) %&amp;gt;%
  as.data.frame(.) %&amp;gt;%
  dplyr::rename_all(funs(c(paste0(&amp;quot;classic_&amp;quot;,.))))

ts_sim_boot_wall &amp;lt;- ts_sim_wall_run %&amp;gt;%
  tseries::tsbootstrap(., nb=5, b=350, type = &amp;quot;block&amp;quot;) %&amp;gt;%
  as.data.frame(.) %&amp;gt;%
  dplyr::rename_all(funs(c(paste0(&amp;quot;wall_&amp;quot;,.))))

ts_sim_df &amp;lt;- cbind(ts_sim_boot_classic,ts_sim_boot_wall)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;heatmap-cluster&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Heatmap cluster&lt;/h1&gt;
&lt;p&gt;Even if I’m a big fan of ggplot2 possibilities, some packages offer efficient ways to compute and plot data. For heatmaps I’m using the &lt;code&gt;gplots&lt;/code&gt; package which displays time series with dendrograms is a single function. An overlook of all the heatmap possibilities can be found &lt;a href=&#34;http://www.sthda.com/english/articles/28-hierarchical-clustering-essentials/93-heatmap-static-and-interactive-absolute-guide/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dtw_dist &amp;lt;- function(x){dist(x, method=&amp;quot;DTW&amp;quot;)}

ts_sim_df %&amp;gt;%
  as.matrix() %&amp;gt;%
  gplots::heatmap.2 (
    # dendrogram control
    distfun = dtw_dist,
    hclustfun = hclust,
    dendrogram = &amp;quot;column&amp;quot;,
    Rowv = FALSE,
    labRow = FALSE
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-06-time-series-clustering-with-dynamic-time-warp_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can already see an accurate clustering between classic and wall runs but we are interested in DTW analysis so let’s implement &lt;code&gt;TSclust&lt;/code&gt; and &lt;code&gt;dtwclust&lt;/code&gt; packages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dtw-cluster&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;DTW cluster&lt;/h1&gt;
&lt;p&gt;Both &lt;code&gt;TSclust&lt;/code&gt; and &lt;code&gt;dtwclust&lt;/code&gt; are following the same steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Calculating the difference between each time series using the DTW method (but many other distances can be calculated, see for example Montero &amp;amp; Vilar, 2014).&lt;/li&gt;
&lt;li&gt;Calculating hierarchical cluster analysis over these dissimilarities.&lt;/li&gt;
&lt;li&gt;Plotting a dendrogram to visually assess the cluster accuracy. The solution to plot the time series with the dendrogram was taken from &lt;a href=&#34;http://www.hanselsolutions.com/blog/clustering-time-series.html&#34;&gt;Ian Hansel’s blog&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;using-tsclust&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using &lt;code&gt;TSclust&lt;/code&gt;&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# cluster analysis
dist_ts &amp;lt;- TSclust::diss(SERIES = t(ts_sim_df), METHOD = &amp;quot;DTWARP&amp;quot;) # note the dataframe must be transposed
hc &amp;lt;- stats::hclust(dist_ts, method=&amp;quot;complete&amp;quot;) # meathod can be also &amp;quot;average&amp;quot; or diana (for DIvisive ANAlysis Clustering)
# k for cluster which is 2 in our case (classic vs. wall)
hclus &amp;lt;- stats::cutree(hc, k = 2) %&amp;gt;% # hclus &amp;lt;- cluster::pam(dist_ts, k = 2)$clustering has a similar result
  as.data.frame(.) %&amp;gt;%
  dplyr::rename(.,cluster_group = .) %&amp;gt;%
  tibble::rownames_to_column(&amp;quot;type_col&amp;quot;)

hcdata &amp;lt;- ggdendro::dendro_data(hc)
names_order &amp;lt;- hcdata$labels$label
# Use the folloing to remove labels from dendogram so not doubling up - but good for checking hcdata$labels$label &amp;lt;- &amp;quot;&amp;quot;

p1 &amp;lt;- hcdata %&amp;gt;%
  ggdendro::ggdendrogram(., rotate=TRUE, leaf_labels=FALSE)

p2 &amp;lt;- ts_sim_df %&amp;gt;%
  dplyr::mutate(index = 1:420) %&amp;gt;%
  tidyr::gather(key = type_col,value = value, -index) %&amp;gt;%
  dplyr::full_join(., hclus, by = &amp;quot;type_col&amp;quot;) %&amp;gt;% 
  mutate(type_col = factor(type_col, levels = rev(as.character(names_order)))) %&amp;gt;% 
  ggplot(aes(x = index, y = value, colour = cluster_group)) +
  geom_line() +
  facet_wrap(~type_col, ncol = 1, strip.position=&amp;quot;left&amp;quot;) + 
  guides(color=FALSE) +
  theme_bw() + 
  theme(strip.background = element_blank(), strip.text = element_blank())

gp1&amp;lt;-ggplotGrob(p1)
gp2&amp;lt;-ggplotGrob(p2) 

grid.arrange(gp2, gp1, ncol=2, widths=c(4,2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-06-time-series-clustering-with-dynamic-time-warp_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, The results of &lt;code&gt;TSclust&lt;/code&gt;show two different groups, one with the classic runs and one with wall runs. However we can see that wall runs are not sorted perfectly according their shape. Let’s have a look at &lt;code&gt;dtwclust&lt;/code&gt; to see if the results are similar.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-dtwclust&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using &lt;code&gt;dtwclust&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The main asset of &lt;code&gt;dtwclust&lt;/code&gt; is the possibility to customize the DTW clustering. For more details about all the possibilities, I suggest to have a look at the &lt;code&gt;dtwclust&lt;/code&gt; package &lt;a href=&#34;https://cran.r-project.org/web/packages/dtwclust/vignettes/dtwclust.pdf&#34;&gt;vignette&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cluster_dtw_h2 &amp;lt;- dtwclust::tsclust(t(ts_sim_df), 
                                    type = &amp;quot;h&amp;quot;, 
                                    k = 2,  
                                    distance = &amp;quot;dtw&amp;quot;, 
                                    control = hierarchical_control(method = &amp;quot;complete&amp;quot;),
                                    preproc = NULL, 
                                    args = tsclust_args(dist = list(window.size = 5L)))

hclus &amp;lt;- stats::cutree(cluster_dtw_h2, k = 2) %&amp;gt;% # hclus &amp;lt;- cluster::pam(dist_ts, k = 2)$clustering has a similar result
  as.data.frame(.) %&amp;gt;%
  dplyr::rename(.,cluster_group = .) %&amp;gt;%
  tibble::rownames_to_column(&amp;quot;type_col&amp;quot;)

hcdata &amp;lt;- ggdendro::dendro_data(cluster_dtw_h2)
names_order &amp;lt;- hcdata$labels$label
# Use the folloing to remove labels from dendogram so not doubling up - but good for checking hcdata$labels$label &amp;lt;- &amp;quot;&amp;quot;

p1 &amp;lt;- hcdata %&amp;gt;%
  ggdendro::ggdendrogram(., rotate=TRUE, leaf_labels=FALSE)

p2 &amp;lt;- ts_sim_df %&amp;gt;%
  dplyr::mutate(index = 1:420) %&amp;gt;%
  tidyr::gather(key = type_col,value = value, -index) %&amp;gt;%
  dplyr::full_join(., hclus, by = &amp;quot;type_col&amp;quot;) %&amp;gt;% 
  mutate(type_col = factor(type_col, levels = rev(as.character(names_order)))) %&amp;gt;% 
  ggplot(aes(x = index, y = value, colour = cluster_group)) +
  geom_line() +
  facet_wrap(~type_col, ncol = 1, strip.position=&amp;quot;left&amp;quot;) + 
  guides(color=FALSE) +
  theme_bw() + 
  theme(strip.background = element_blank(), strip.text = element_blank())

gp1&amp;lt;-ggplotGrob(p1)
gp2&amp;lt;-ggplotGrob(p2) 

grid.arrange(gp2, gp1, ncol=2, widths=c(4,2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-06-time-series-clustering-with-dynamic-time-warp_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now with the cluster are well distributed between classic and wall runs but also inside the clusters where similar shapes appears to be grouped together.&lt;/p&gt;
&lt;p&gt;It is possible to modify some argument in order to perform this hierarchical DTW clustering based on z-scores with centroid based on the built-in “shape_extraction” function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cluster_dtw_h2 &amp;lt;- dtwclust::tsclust(t(ts_sim_df), type = &amp;quot;h&amp;quot;, k = 2L,
                                    preproc = zscore,
                                    distance = &amp;quot;dtw&amp;quot;, centroid = shape_extraction,
                                    control = hierarchical_control(method = &amp;quot;complete&amp;quot;))

hclus &amp;lt;- stats::cutree(cluster_dtw_h2, k = 2) %&amp;gt;% # hclus &amp;lt;- cluster::pam(dist_ts, k = 2)$clustering has a similar result
  as.data.frame(.) %&amp;gt;%
  dplyr::rename(.,cluster_group = .) %&amp;gt;%
  tibble::rownames_to_column(&amp;quot;type_col&amp;quot;)

hcdata &amp;lt;- ggdendro::dendro_data(cluster_dtw_h2)
names_order &amp;lt;- hcdata$labels$label
# Use the folloing to remove labels from dendogram so not doubling up - but good for checking hcdata$labels$label &amp;lt;- &amp;quot;&amp;quot;

p1 &amp;lt;- hcdata %&amp;gt;%
  ggdendro::ggdendrogram(., rotate=TRUE, leaf_labels=FALSE)

p2 &amp;lt;- ts_sim_df %&amp;gt;%
  dplyr::mutate(index = 1:420) %&amp;gt;%
  tidyr::gather(key = type_col,value = value, -index) %&amp;gt;%
  dplyr::full_join(., hclus, by = &amp;quot;type_col&amp;quot;) %&amp;gt;% 
  mutate(type_col = factor(type_col, levels = rev(as.character(names_order)))) %&amp;gt;% 
  ggplot(aes(x = index, y = value, colour = cluster_group)) +
  geom_line() +
  facet_wrap(~type_col, ncol = 1, strip.position=&amp;quot;left&amp;quot;) + 
  guides(color=FALSE) +
  theme_bw() + 
  theme(strip.background = element_blank(), strip.text = element_blank())

gp1&amp;lt;-ggplotGrob(p1)
gp2&amp;lt;-ggplotGrob(p2) 

grid.arrange(gp2, gp1, ncol=2, widths=c(4,2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-06-time-series-clustering-with-dynamic-time-warp_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on &lt;code&gt;dtwclust&lt;/code&gt; package vignette, it is possible to register a new DTW function adapted to normalized and asymmetric DTW.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Normalized DTW
ndtw &amp;lt;- function(x, y, ...) {
  dtw(x, y, ...,
      step.pattern = asymmetric,
      distance.only = TRUE)$normalizedDistance
}
# Register the distance with proxy
proxy::pr_DB$set_entry(FUN = ndtw, names = c(&amp;quot;nDTW&amp;quot;),
                       loop = TRUE, type = &amp;quot;metric&amp;quot;, distance = TRUE,
                       description = &amp;quot;Normalized, asymmetric DTW&amp;quot;)
# Partitional clustering
cluster_dtw_h2 &amp;lt;- dtwclust::tsclust(t(ts_sim_df), k = 2L,distance = &amp;quot;nDTW&amp;quot;)

plot(cluster_dtw_h2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-06-time-series-clustering-with-dynamic-time-warp_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even if it looks great with &lt;code&gt;sine&lt;/code&gt; simulated data, it is not very accurate with ARIMA models. Moreover I haven’t been able to extract the dendrogram from this last “cluster_dtw_h2” object because of the partitional clustering process but one can be interested in the distance matrix provided in “cluster_dtw_h2” object.&lt;/p&gt;
&lt;p&gt;After this short analysis with Dynamic Time Warping, the next steps will be to increase the difference between the time series to check the clustering accuracy and obviously to test it with real data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Publishing blogdown, some insights from my own experience</title>
      <link>/post/publishing-blogdown-some-insights-from-my-own-experience/</link>
      <pubDate>Tue, 04 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/publishing-blogdown-some-insights-from-my-own-experience/</guid>
      <description>


&lt;p&gt;Finally! My first post to my website is about to be live on the web! I’m always amazed by the possibilities offered by R and Rstudio and my first lines will thank this huge community which make the awesomeness real. However it’s not always easy to use and to apply these magnificent tools. Publishing a blogdown is a good example of a process that looks easy on the first sight but which can be tricky. Here are some insights from my experience in publishing a research blog from blogdown.&lt;/p&gt;
&lt;div id=&#34;first-steps-with-blogdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;First steps with blogdown&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;blogdown&lt;/code&gt; package is very well documented and if you are intended to publish your own websites the best approach is to have a look at the bookdown made by Yihui Xie, Amber Thomas and Alison Presmanes Hill &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;blogdown: Creating Websites with R Markdown&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Several tutorials are available to help new users in this case:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A blog post writen by Alison Presmanes Hill, &lt;a href=&#34;https://alison.rbind.io/post/up-and-running-with-blogdown/&#34;&gt;Up and running with blogdown&lt;/a&gt;, is a very good first step to blogdown. The post is well documented and will help to solve most of the major problems to setup a website.&lt;/li&gt;
&lt;li&gt;The talk given by Yihui Xie at the rstudio::conf 2018, &lt;a href=&#34;https://www.rstudio.com/resources/videos/create-and-maintain-websites-with-r-markdown-and-blogdown/&#34;&gt;Create and maintain websites with R Markdown and blogdown&lt;/a&gt;, is short and very well presented.&lt;/li&gt;
&lt;li&gt;The youtube tutorial by John Muschelli, &lt;a href=&#34;https://www.youtube.com/watch?v=syWAKaj-4ck&#34;&gt;Making a Website with Blogdown&lt;/a&gt;, presents all the steps from building a website to publishing it on netlify.com.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-solving&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Problem solving&lt;/h2&gt;
&lt;p&gt;Even if the authors of &lt;code&gt;blogdown&lt;/code&gt; made it very easy to publish a blog from Rstudio, some problem can be encountered while following the basic steps. Here is my experience in troubleshooting some of the problems that can happen.&lt;/p&gt;
&lt;div id=&#34;prerequisite&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prerequisite&lt;/h3&gt;
&lt;p&gt;In order to avoid the majority of the problems that may happen, it is essential to have the latest versions of R, Rstudio, &lt;code&gt;blogdown&lt;/code&gt; package (don’t hesitate to use its GitHub version &lt;code&gt;devtools::install_github(&amp;quot;rstudio/blogdown&amp;quot;)&lt;/code&gt;) and Hugo (&lt;code&gt;blogdown::update_hugo()&lt;/code&gt;). Like mine, &lt;strong&gt;if the process of publishing a blogdown takes several weeks, upgrading to the latest version for every try can slove a lot of problems&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-github-connection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating a GitHub connection&lt;/h3&gt;
&lt;p&gt;If one wants to make blogdown live on the web, a GitHub integration is the way to go according to me. Two routes are possible:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Creating a blogdown project from Rstudio and then uploading the project on GitHub. Rstudio GUI is one of its main asset and creating a blogdown project from Rstudio is very easy but the connection with GitHub has to be done afterward which can lead to several problems when committing the changes.&lt;/li&gt;
&lt;li&gt;Creating an empty repo on GitHub, then creating a new project with version control on Rstudio and use the function &lt;code&gt;blogdown::build_site()&lt;/code&gt; in this new project. &lt;strong&gt;This solution doesn’t look as straight forward but once it is done, the connection with GitHub is very stable&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-a-new-theme&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Installing a new theme&lt;/h3&gt;
&lt;p&gt;When it comes to build the site, it is possible to specify the theme you want to use with &lt;code&gt;blogdown::new_site(theme = &amp;quot;gcushen/hugo-academic&amp;quot;)&lt;/code&gt;. Even if the default theme is nice, researchers and students can find the theme “academic” more suitable. But be sure to have installed a version of Hugo that corresponds with the theme version of Hugo.&lt;/p&gt;
&lt;p&gt;After this initial command, the template website should be displayed in Rstudio viewer. If you have to close the project it is possible to relaunch this view using the Serve Site shortcut in Rstudio Addins or by using &lt;code&gt;blogdown::serve_site()&lt;/code&gt;. Next step is the uploading of this template website in GitHub.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;making-github-commit-and-push&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Making GitHub commit and push&lt;/h3&gt;
&lt;p&gt;A big advantage of GitHub integration is that when your website is live on the web, any new post or new content is automatically uploaded with simple commit and push from Rstudio project. Rstudio provides a nice interface to do these commit and push to GitHub without using any command lines which is great for new coders and/or windows users. However with the initial commit and push of a blogdown I didn’t manage to use Rstudio GUI resulting of endless lags and multiple reboots. &lt;strong&gt;The solution is to use github commit and push from command lines&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;if you are new to command lines don’t be afraid, it is very easy:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If your project is linked with GitHub, that means Git is also installed and Git comes with a CMD prompt that can be used for manual commit and push to GitHub.&lt;/li&gt;
&lt;li&gt;Change the directory
&lt;ul&gt;
&lt;li&gt;from &lt;code&gt;C:\Users\MyName&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;to &lt;code&gt;C:\Users\MyName\MyFolderName\MyBlogdownProjectName&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;using &lt;code&gt;cd .\MyFolderName\MyBlogdownProjectName&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Add all changes with the command &lt;code&gt;git add -A&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Commit these changes with the command &lt;code&gt;git commit -m &amp;quot;initial commit text&amp;quot;&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Push these changes to GitHub with the command &lt;code&gt;git push&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note: these commands are the most basic ones, it can me more complicated to add only specific files or to push from another branch but in these cases you can easily find the commands on the web.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;publishing-the-template-to-netlify&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Publishing the template to Netlify&lt;/h3&gt;
&lt;p&gt;Once this initial commit is done, it is possible to make the template live on Netlify. I think it is a good idea to not modify the template yet in order to first check how Netlify is handling the template website.&lt;/p&gt;
&lt;p&gt;On Netlify, with GitHub login and password, it is easy to find you repo and to deploy the website. However Netlify will analyse the project and this can result in more errors and problems. The biggest problem that I had was solved very easily with the genius Mara Averick and her blog post &lt;a href=&#34;https://maraaverick.rbind.io/2017/10/updating-blogdown-hugo-version-netlify/&#34;&gt;Updating your version of Hugo for blogdown on Netlify&lt;/a&gt;. I should create a blog dedicated to how amazing Mara is and as usual she solved a massive problem. Indeed, Hugo’s theme have specific minimum Hugo version to use but even Rstudio is using the latest version to build the website, Netlify needs to know which version to use. &lt;strong&gt;The correct version must be set as a New variable with the key to &lt;code&gt;HUGO_VERSION&lt;/code&gt;&lt;/strong&gt;. Then the magic happens and the website is deployed to the world. the first URL address of the website is quite complicated but Netlify allows to change it for free.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;changing-the-content-of-the-template&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Changing the content of the template&lt;/h3&gt;
&lt;p&gt;Last but not least, the website you have just published needs to be filled with your own content. The first step will be to replace all the lines in the sub-folder &lt;code&gt;home&lt;/code&gt; with your own data and to delete all the posts, publications, citations, images, etc. inside the template. However it is still possible to broke the website by deleting a useful link. In this case I have two advice:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Keep a version of the template in case you need to get back one of the files and then understanding why deleting these information is braking the website.&lt;/li&gt;
&lt;li&gt;Do not delete the files called &lt;code&gt;_index.md&lt;/code&gt;, they are very useful to create the public site of the website.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-next&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What’s next?&lt;/h2&gt;
&lt;p&gt;The websites created with the &lt;code&gt;blogdown&lt;/code&gt; package from the R and stats community are united in a project called &lt;a href=&#34;https://github.com/rbind?tab=repositories&#34;&gt;rbind&lt;/a&gt;. I hope to be one day expert enough to be part of this community and hopefully with my tricks and tips, you will as well ;)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The effect of personality and social context on willingness to share emotion information on social media</title>
      <link>/publication/2018-socialmedia-psyarxiv/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/2018-socialmedia-psyarxiv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Role of emotions in product acceptance: Evaluation of the cognitive, motivational and subjective component</title>
      <link>/publication/2017-acceptance-pto/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/2017-acceptance-pto/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Comparison of Three Commercial Systems for Automatic Recognition of Spontaneous Facial Expressions</title>
      <link>/talk/2018-cere/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/talk/2018-cere/</guid>
      <description>&lt;p&gt;SYMPOSIUM 2: Affect Recognition in Humans versus Machines: Current Issues and Future Challenges
&lt;em&gt;Convener: Eva Krumhuber, University College London, UK&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ABSTRACT: Automatic facial expression recognition systems can provide important information about our emotions and how they change over time. While the use of automatic systems has seen a steady increase over the last years, their classification results have not yet been systematically compared. The aim of this research was to test commercial software packages from Affectiva, Kairos and Microsoft companies in terms of their recognition accuracy. For this, we focused on spontaneous and dynamic facial expressions as provided by the DynEmo database –Disgust, Fear, Joy, and Surprise. In order to compare the classification results, we calculated the systems’ ratio of True positives (only the target label is recognised), False positives (the target label as well as a non-target label is recognised), True negatives (no label is recognised) and False negatives (target label is not recognised whereas a non-target label is).
The results of the comparison between the systems showed comparable detection rates in term of True positives and False positives. However, their detection rates of False negatives and True negatives significantly differed between the different recognition systems. Specifically, systems were not equal in their tendency to detect non-target labels erroneously as well as in their tendency to not detect any emotion label. When examining emotion recognition accuracy for each video/emotion, videos with higher recognition accuracy were those that depicted as joyful facial expression. Other facial expressions resulted in a proportion of target emotion detection statistically equal or lower than the detection of non-target emotion.
These results suggest that systems are not equivalent in their ability to detect specific spontaneous emotions. Therefore, users of such systems have to be aware of the strengths as well as of the potential limits of the data provided by automatic emotion recognition systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accuracy of three commercial automatic emotion recognition systems across different individuals and their facial expressions</title>
      <link>/talk/2018-percom/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/talk/2018-percom/</guid>
      <description>&lt;p&gt;ABSTRACT:&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Are approach-avoidance relevant cues of the Emotional User eXperience? Case studies with innovative products</title>
      <link>/talk/2014-cere/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/talk/2014-cere/</guid>
      <description>&lt;p&gt;ABSTRACT:&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Analysis of Automatic Emotion Recognition Using Generalized Additive Mixed Models</title>
      <link>/talk/2017-aisb/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/talk/2017-aisb/</guid>
      <description>&lt;p&gt;ABSTRACT: With the development of wearable sensors, it is now possible to assess the dynamic progression of physiological rhythms such as heart rate, breathing rate or galvanic skin response in ways and places and during certain activities that were previously not possible due to cost and reliability. This paper investigates the physiological changes when participating in a zipline activity. Despite the advances in sensor technology, the statistical analysis of such physiological signals remains a challenge for data analysts. This paper presents a workflow encompassing the whole process with the goal of obtaining a range of best fitting models to analyse the patterns given by these measurements “in the wild”.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Analysis of Automatic Facial Expressions Recognition ‘in the Wild’ Using Generalized Additive Mixed Models and Significant Zero Crossing of the Derivatives</title>
      <link>/talk/2018-bhci/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/talk/2018-bhci/</guid>
      <description>&lt;p&gt;ABSTRACT: The analysis of facial expressions is currently a favoured method of inferring experienced emotion, and consequently significant efforts are currently being made to develop improved facial expression recognition techniques. Among these new techniques, those which allow the automatic recognition of facial expression appear to be most promising. This paper presents a new method of facial expression analysis with a focus on the continuous evolution of emotions using Generalized Additive Mixed Models (GAMM) and Significant Zero Crossing of the Derivatives (SiZer). The time-series analysis of the emotions experienced by participants watching a series of three different online videos suggests that analysis of facial expressions at the overall level may lead to misinterpretation of the emotional experience whereas non-linear analysis allows the significant expressive sequences to be identified.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Model of Athletes’ Emotions Based on Wearable Devices</title>
      <link>/talk/2017-ahfe/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/talk/2017-ahfe/</guid>
      <description>&lt;p&gt;ABSTRACT: With the development of wearable sensors, it is now possible to assess the dynamic progression of physiological rhythms such as heart rate, breathing rate or galvanic skin response in ways and places that were previously impractical. This paper presents a new application that synchronizes the emotional patterns from these time-series in order to model athletes’ emotion during physical activity. This data analysis computes a best-fitting model for analyzing the patterns given by these measurements “in the wild”. The recording setup used to measure and synchronize multiple biometric physiological sensors can be called a BAN (Body Area Network) of personal measurements. By monitoring physical activity, it is now possible to calculate optimal patterns for managing athletes’ emotion. The data provided here are not restricted by a lab environment but close to the “ground truth” of ecologically valid physiological changes. The data allow the provision of accurate feedback to athletes about their emotion (e.g. in cases such as an unexpected increase or an expected decrease of physiological activity).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Emotions triggered by innovative products, A multi-componential approach of emotions for User eXperience tools</title>
      <link>/talk/2015-acii/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/talk/2015-acii/</guid>
      <description>&lt;p&gt;ABSTRACT: User eXperience studies with products, systems or services have significantly increased in companies in order to anticipate their commercial success. Among the user experience dimensions, emotions are predominant. However User eXperience studies used several concepts to refer to emotions and current measures still have some flaws. Consequently, this doctoral project aims firstly to provide a multi-componential approach of emotions based on a psychological view, and secondly to provide Affective Computing solutions in order to evaluate emotions in User eXperience studies. Through a study using hand-gesture interface devices, three components of users&amp;rsquo; emotions were simultaneously measured with self-reports: the subjective, cognitive and motivational components. The results point out the possibility of measuring different components in order to gain a better understanding of emotions triggered by products. They also point out that self-reports measures could be improved with Affective Computing solutions. In this perspective, two emotion assessment tools were developed: Oudjat and EmoLyse.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
