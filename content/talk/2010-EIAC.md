+++
date = 2017-01-01T00:00:00  # Schedule page publish date.

title = "Interface faciale émotionnelle : Les effets des différentes modalités de presentation"
time_start = 2010-10-13T12:00:00
time_end = 2010-10-15T12:00:00
abstract = ""
abstract_short = ""
event = "Ergonomie et Informatique Avancee Conference - Ergo'IA '10"
event_url = "https://app.dimensions.ai/details/publication/pub.1018888640"
location = "Biarritz, France"

# Is this a selected talk? (true/false)
selected = false

# Projects (optional).
#   Associate this talk with one or more of your projects.
#   Simply enter the filename (excluding '.md') of your project file in `content/project/`.
#projects = ["deep-learning"]

# Links (optional).
url_pdf = ""
url_slides = ""
url_video = ""
url_code = ""

# Does the content use math formatting?
math = true

# Does the content use source code highlighting?
highlight = true

# Featured image
# Place your image in the `static/img/` folder and reference its filename below, e.g. `image = "example.jpg"`.
[header]
image = ""
caption = ""

+++

ABSTRACT: In supporting the visual-mediated emotional recognition, little research has centred on analysing the effect of presenting different combinatorial facial designs. Moreover, in the theoretical field of emotions, research on the recognition of emotional facial expressions (EFE) is mainly based on static and posed databases tested in unnatural contexts (explicit recognition task of an emotion). Our research has constructed and validated a database, DynEmo, with dynamic and spontaneous emotional facial expressions. So different facial interface designs (whole face, zoomed face, eyes + mouth, whole face + mouth, whole face + eyes, etc., n = 11) are experimentally compared to find their impact in terms of emotional recognition. The results show the facial interface design relevance in terms of emotional recognition. The application of this work is transferable to facial interfaces useful for video-mediated interaction.
